{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import RefineFormer3D\n",
    "from losses import RefineFormer3DLoss\n",
    "from optimizer import get_optimizer, get_scheduler\n",
    "from metrics import dice_coefficient, iou_score, precision_recall_f1, hausdorff_distance\n",
    "from dataset import BraTSDataset\n",
    "from augmentation import Compose3D, RandomFlip3D, RandomRotation3D, RandomNoise3D\n",
    "from config import DEVICE, IN_CHANNELS, NUM_CLASSES, BASE_LR, WEIGHT_DECAY, NUM_EPOCHS\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, scaler):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, targets in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # Mixed precision\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    return epoch_loss\n",
    "\n",
    "def validate_one_epoch(model, dataloader, criterion, device, num_classes):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    dice_all, iou_all, hausdorff_all = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader, desc=\"Validation\"):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            with autocast():  # Optional mixed precision inference\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "            preds = outputs[\"main\"]\n",
    "\n",
    "            dice_scores, _ = dice_coefficient(preds, targets, num_classes)\n",
    "            iou_scores, _ = iou_score(preds, targets, num_classes)\n",
    "            hausdorff = hausdorff_distance(preds, targets)\n",
    "\n",
    "            dice_all.append(np.mean(dice_scores))\n",
    "            iou_all.append(np.mean(iou_scores))\n",
    "            hausdorff_all.append(hausdorff)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    mean_dice = np.nanmean(dice_all)\n",
    "    mean_iou = np.nanmean(iou_all)\n",
    "    mean_hausdorff = np.nanmean(hausdorff_all)\n",
    "\n",
    "    return epoch_loss, mean_dice, mean_iou, mean_hausdorff\n",
    "\n",
    "def save_checkpoint(state, save_dir, filename=\"best_model.pth\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save(state, os.path.join(save_dir, filename))\n",
    "\n",
    "def main():\n",
    "    # ==================== CONFIG ====================\n",
    "    device = torch.device(DEVICE)\n",
    "    save_dir = \"./checkpoints\"\n",
    "    best_dice = 0.0\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # ==================== MODEL ====================\n",
    "    model = RefineFormer3D(in_channels=IN_CHANNELS, num_classes=NUM_CLASSES)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # ==================== OPTIMIZER + LOSS ====================\n",
    "    optimizer = get_optimizer(model, base_lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = get_scheduler(optimizer, mode=\"cosine\", T_max=NUM_EPOCHS)\n",
    "    criterion = RefineFormer3DLoss()\n",
    "\n",
    "    # ==================== DATASET + DATALOADER ====================\n",
    "    train_transform = Compose3D([\n",
    "        RandomFlip3D(p=0.5),\n",
    "        RandomRotation3D(p=0.5),\n",
    "        RandomNoise3D(p=0.3),\n",
    "    ])\n",
    "\n",
    "    train_dataset = BraTSDataset(\n",
    "    root_dirs=[\n",
    "        \"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17TrainingData/HGG\",   # 🔥 Replace this\n",
    "        \"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17TrainingData/LGG\"    # 🔥 Replace this\n",
    "    ],\n",
    "    transform=train_transform,\n",
    "    )   \n",
    "\n",
    "    val_dataset = BraTSDataset(\n",
    "    root_dirs=[\n",
    "        \"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData\"      # 🔥 Replace this\n",
    "    ],\n",
    "    transform=None,\n",
    "    )\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # ==================== TRAINING LOOP ====================\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        print(f\"\\n--- Epoch [{epoch}/{NUM_EPOCHS}] ---\")\n",
    "\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, scaler)\n",
    "        val_loss, val_dice, val_iou, val_hd = validate_one_epoch(model, val_loader, criterion, device, NUM_CLASSES)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Dice: {val_dice:.4f} | Val IoU: {val_iou:.4f} | Hausdorff: {val_hd:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_dice > best_dice:\n",
    "            best_dice = val_dice\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_dice': best_dice,\n",
    "            }, save_dir)\n",
    "            print(f\"✅ Saved Best Model (Dice: {best_dice:.4f})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3164226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b6e7fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50727/3217983263.py:75: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total valid patient directories found: 411\n",
      "✅ Total valid patient directories found: 73\n",
      "\n",
      "--- Epoch [1/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/206 [00:00<?, ?it/s]/tmp/ipykernel_50727/3217983263.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Training crashed due to: only one dimension can be inferred\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "from model2 import RefineFormer3D\n",
    "from dataset import BraTSDataset\n",
    "from optimizer import get_optimizer, get_scheduler\n",
    "from metrics import dice_coefficient, iou_score, hausdorff_distance\n",
    "from augmentation import Compose3D, RandomFlip3D, RandomRotation3D, RandomNoise3D\n",
    "from losses import RefineFormer3DLoss\n",
    "from config import DEVICE, IN_CHANNELS, NUM_CLASSES, BASE_LR, WEIGHT_DECAY, NUM_EPOCHS\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, scaler):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, targets in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device=device, dtype=torch.long, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(inputs)['main']\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, dataloader, criterion, device, num_classes):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    dice_all, iou_all, hd_all = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device=device, dtype=torch.long, non_blocking=True)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            dice_all.append(dice_coefficient(preds, targets, num_classes))\n",
    "            iou_all.append(iou_score(preds, targets, num_classes))\n",
    "            hd_all.append(hausdorff_distance(preds, targets))\n",
    "\n",
    "    val_loss = running_loss / len(dataloader.dataset)\n",
    "    avg_dice = torch.mean(torch.stack(dice_all)).item()\n",
    "    avg_iou = torch.mean(torch.stack(iou_all)).item()\n",
    "    avg_hd = torch.mean(torch.stack(hd_all)).item()\n",
    "\n",
    "    return val_loss, avg_dice, avg_iou, avg_hd\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = DEVICE\n",
    "    model = RefineFormer3D(in_channels=IN_CHANNELS, num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "    optimizer = get_optimizer(model, base_lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "    criterion = RefineFormer3DLoss()\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    train_transform = Compose3D([\n",
    "        RandomFlip3D(p=0.5),\n",
    "        RandomRotation3D(p=0.5),\n",
    "        RandomNoise3D(p=0.3),\n",
    "    ])\n",
    "\n",
    "    # Use new split structure\n",
    "    train_dataset = BraTSDataset(\n",
    "        root_dirs=[\"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BRATS_SPLIT/train\"],\n",
    "        transform=train_transform,\n",
    "    )\n",
    "\n",
    "    val_dataset = BraTSDataset(\n",
    "        root_dirs=[\"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BRATS_SPLIT/val\"],\n",
    "        transform=None,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    try:\n",
    "        for epoch in range(1, NUM_EPOCHS + 1):\n",
    "            print(f\"\\n--- Epoch [{epoch}/{NUM_EPOCHS}] ---\")\n",
    "            train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, scaler)\n",
    "            val_loss, val_dice, val_iou, val_hd = validate_one_epoch(model, val_loader, criterion, device, NUM_CLASSES)\n",
    "            scheduler.step()\n",
    "            print(f\"Train Loss: {train_loss:.4f}  |  Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"Val Dice: {val_dice:.4f} | Val IOU: {val_iou:.4f} | Val Hausdorff: {val_hd:.4f}\")\n",
    "            torch.save(model.state_dict(), f\"checkpoint_epoch_{epoch}.pt\")\n",
    "    except Exception as e:\n",
    "        print(f\"🚨 Training crashed due to: {e}\")\n",
    "        torch.save(model.state_dict(), \"crashed_model2.pt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c74e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import GradScaler, autocast \n",
    "from tqdm import tqdm\n",
    "from model4 import RefineFormer3D\n",
    "from dataset import BraTSDataset\n",
    "from optimizer import get_optimizer, get_scheduler\n",
    "from metrics import dice_coefficient, iou_score, hausdorff_distance\n",
    "from augmentation import Compose3D, RandomFlip3D, RandomRotation3D, RandomNoise3D\n",
    "from losses import RefineFormer3DLoss\n",
    "from config import DEVICE, IN_CHANNELS, NUM_CLASSES, BASE_LR, WEIGHT_DECAY, NUM_EPOCHS\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, scaler):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, targets in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device=device, dtype=torch.long, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(inputs)['main']\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "def validate_one_epoch(model, dataloader, criterion, device, num_classes):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    dice_all, iou_all, hd_all = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device=device, dtype=torch.long, non_blocking=True)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            preds = torch.argmax(outputs['main'], dim=1)\n",
    "            dice_all.append(dice_coefficient(preds, targets, num_classes))\n",
    "            iou_all.append(iou_score(preds, targets, num_classes))\n",
    "            hd_all.append(hausdorff_distance(preds, targets))\n",
    "\n",
    "    val_loss = running_loss / len(dataloader.dataset)\n",
    "    return val_loss, torch.mean(torch.stack(dice_all)).item(), torch.mean(torch.stack(iou_all)).item(), torch.mean(torch.stack(hd_all)).item()\n",
    "\n",
    "def main():\n",
    "    device = DEVICE\n",
    "    model = RefineFormer3D(in_channels=IN_CHANNELS, num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "    optimizer = get_optimizer(model, base_lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "    criterion = RefineFormer3DLoss()\n",
    "    scaler = GradScaler(device='cuda')  # Updated for deprecation\n",
    "\n",
    "    train_transform = Compose3D([\n",
    "        RandomFlip3D(p=0.5),\n",
    "        RandomRotation3D(p=0.5),\n",
    "        RandomNoise3D(p=0.3),\n",
    "    ])\n",
    "\n",
    "    train_dataset = BraTSDataset(\n",
    "        root_dirs=[\"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BRATS_SPLIT/train\"],\n",
    "        transform=train_transform,\n",
    "    )\n",
    "    val_dataset = BraTSDataset(\n",
    "        root_dirs=[\"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BRATS_SPLIT/val\"],\n",
    "        transform=None,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "\n",
    "        print(f\"\\n--- Epoch [{epoch}/{NUM_EPOCHS}] ---\")\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, scaler)\n",
    "        val_loss, val_dice, val_iou, val_hd = validate_one_epoch(model, val_loader, criterion, device, NUM_CLASSES)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}  |  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Val Dice: {val_dice:.4f} | Val IOU: {val_iou:.4f} | Val Hausdorff: {val_hd:.4f}\")\n",
    "        torch.save(model.state_dict(), f\"checkpoint_epoch_{epoch}.pt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd1827a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total valid patient directories found: 411\n",
      "\n",
      "--- Epoch [1/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7553\n",
      "\n",
      "--- Epoch [2/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2305\n",
      "\n",
      "--- Epoch [3/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8960\n",
      "\n",
      "--- Epoch [4/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6988\n",
      "\n",
      "--- Epoch [5/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5947\n",
      "\n",
      "--- Epoch [6/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5332\n",
      "\n",
      "--- Epoch [7/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4961\n",
      "\n",
      "--- Epoch [8/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4596\n",
      "\n",
      "--- Epoch [9/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4406\n",
      "\n",
      "--- Epoch [10/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4233\n",
      "\n",
      "--- Epoch [11/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4061\n",
      "\n",
      "--- Epoch [12/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3961\n",
      "\n",
      "--- Epoch [13/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3993\n",
      "\n",
      "--- Epoch [14/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3823\n",
      "\n",
      "--- Epoch [15/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3806\n",
      "\n",
      "--- Epoch [16/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3733\n",
      "\n",
      "--- Epoch [17/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3662\n",
      "\n",
      "--- Epoch [18/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3590\n",
      "\n",
      "--- Epoch [19/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3612\n",
      "\n",
      "--- Epoch [20/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3492\n",
      "\n",
      "--- Epoch [21/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3471\n",
      "\n",
      "--- Epoch [22/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3457\n",
      "\n",
      "--- Epoch [23/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3443\n",
      "\n",
      "--- Epoch [24/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3454\n",
      "\n",
      "--- Epoch [25/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3361\n",
      "\n",
      "--- Epoch [26/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3270\n",
      "\n",
      "--- Epoch [27/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3375\n",
      "\n",
      "--- Epoch [28/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3232\n",
      "\n",
      "--- Epoch [29/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3288\n",
      "\n",
      "--- Epoch [30/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "from model4 import RefineFormer3D\n",
    "from dataset import BraTSDataset\n",
    "from optimizer import get_optimizer, get_scheduler\n",
    "from augmentation import Compose3D, RandomFlip3D, RandomRotation3D, RandomNoise3D\n",
    "from losses import RefineFormer3DLoss\n",
    "from config import DEVICE, IN_CHANNELS, NUM_CLASSES, BASE_LR, WEIGHT_DECAY, NUM_EPOCHS\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, scaler):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, targets in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device=device, dtype=torch.long, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(inputs)['main']\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = DEVICE\n",
    "    model = RefineFormer3D(in_channels=IN_CHANNELS, num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "    optimizer = get_optimizer(model, base_lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "    criterion = RefineFormer3DLoss()\n",
    "    scaler = GradScaler(device='cuda')\n",
    "\n",
    "    train_transform = Compose3D([\n",
    "        RandomFlip3D(p=0.5),\n",
    "        RandomRotation3D(p=0.5),\n",
    "        RandomNoise3D(p=0.3),\n",
    "    ])    \n",
    "\n",
    "    train_dataset = BraTSDataset(\n",
    "        root_dirs=[\"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BRATS_SPLIT/train\"],\n",
    "        transform=train_transform,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=12, pin_memory=True)\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        print(f\"\\n--- Epoch [{epoch}/{NUM_EPOCHS}] ---\")\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, scaler)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        torch.save(model.state_dict(), f\"checkpoint_epoch_{epoch}.pt\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"final_model.pt\")\n",
    "    print(\"✅ Final model saved as 'final_model.pt'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2696f67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total valid patient directories found: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46210/862257355.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"checkpoint_epoch_29.pt\", map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (1137, 2), target (19345, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/metrics.py:118: RuntimeWarning: Mean of empty slice\n",
      "  mean_hd = np.nanmean(batch_hd)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (2812, 2), target (61746, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (970, 2), target (18890, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (5358, 2), target (224736, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (2829, 2), target (88007, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (1346, 2), target (37014, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3729, 2), target (123154, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (2363, 2), target (40761, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (5757, 2), target (157587, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3108, 2), target (108885, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3893, 2), target (62599, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (4467, 2), target (109190, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (8326, 2), target (252736, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (4702, 2), target (170865, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3426, 2), target (87207, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (4961, 2), target (91531, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (5031, 2), target (192120, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (4108, 2), target (122497, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (1356, 2), target (20866, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (5230, 2), target (210488, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3522, 2), target (92144, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (7499, 2), target (192556, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (2215, 2), target (44523, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (2428, 2), target (76623, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (4025, 2), target (147952, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (1327, 2), target (8478, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (1877, 2), target (16491, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (1509, 2), target (29360, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (2039, 2), target (31964, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3512, 2), target (70052, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3654, 2), target (89981, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3662, 2), target (122197, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (4247, 2), target (170922, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (4891, 2), target (126496, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3771, 2), target (62221, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3031, 2), target (79664, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3699, 2), target (147371, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (2181, 2), target (83570, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (2979, 2), target (79862, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (4625, 2), target (202182, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (2001, 2), target (63186, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (1137, 2), target (19345, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3726, 2), target (147062, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3036, 2), target (81989, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3707, 2), target (99795, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (2208, 2), target (54335, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (2130, 2), target (36858, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3580, 2), target (99339, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3161, 2), target (134474, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (2590, 2), target (68741, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (1413, 2), target (19150, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3429, 2), target (94618, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (2528, 2), target (59203, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (2444, 2), target (50183, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (2919, 2), target (73657, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (2363, 2), target (37828, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (2318, 2), target (30490, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (2670, 2), target (77196, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (4088, 2), target (156016, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (2212, 2), target (53787, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3002, 2), target (98946, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (4108, 2), target (115199, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (1446, 2), target (42253, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3448, 2), target (126712, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (2661, 2), target (59133, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (677, 2), target (13465, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (4018, 2), target (127238, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3180, 2), target (66672, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (5265, 2), target (170983, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3924, 2), target (133309, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (6017, 2), target (220315, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (3302, 2), target (97135, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "Sample 0 pred shape: (128, 128), target shape: (128, 128, 128)\n",
      "⚠️ Skipping sample_0: shape mismatch (pred (4645, 2), target (193747, 3))\n",
      "ℹ️ Skipped 1 samples due to issues. Writing details to 'skipped_samples.txt'\n",
      "\n",
      "📊 Test Results:\n",
      "Avg Dice Score: 0.8760\n",
      "Avg IOU Score: 0.8051\n",
      "Avg Hausdorff Distance: nan\n",
      "ℹ️ Skipped Hausdorff for 73 samples due to errors.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from model4 import RefineFormer3D\n",
    "from dataset import BraTSDataset\n",
    "from metrics import dice_coefficient, iou_score, hausdorff_distance\n",
    "from config import IN_CHANNELS, NUM_CLASSES\n",
    "\n",
    "DEVICE = torch.device('cpu')\n",
    "\n",
    "# ✅ Load test dataset\n",
    "test_dataset = BraTSDataset(\n",
    "    root_dirs=[\"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BRATS_SPLIT/val\"],\n",
    "    transform=None,\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "# ✅ Initialize model and load weights\n",
    "model = RefineFormer3D(in_channels=IN_CHANNELS, num_classes=NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(\"checkpoint_epoch_29.pt\", map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# ✅ Metric containers\n",
    "dice_all, iou_all, hd_all = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        targets = targets.to(DEVICE, dtype=torch.long)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        raw_preds = outputs['main']\n",
    "        preds = torch.argmax(raw_preds, dim=1)\n",
    "\n",
    "        # Dice + IoU\n",
    "        _, mean_dice = dice_coefficient(raw_preds, targets, NUM_CLASSES)\n",
    "        _, mean_iou = iou_score(raw_preds, targets, NUM_CLASSES)\n",
    "\n",
    "        # Prepare shapes for Hausdorff\n",
    "        pred_input = preds.unsqueeze(0) if preds.dim() == 3 else preds\n",
    "        if targets.dim() == 4 and targets.shape[0] == 1:\n",
    "            target_input = targets.squeeze(0).unsqueeze(0)\n",
    "        elif targets.dim() == 3:\n",
    "            target_input = targets.unsqueeze(0)\n",
    "        else:\n",
    "            target_input = targets\n",
    "\n",
    "        try:\n",
    "            assert pred_input.shape == target_input.shape, f\"Shape mismatch: pred {pred_input.shape}, target {target_input.shape}\"\n",
    "            hd = hausdorff_distance(pred_input, target_input)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ HD skipped for sample {idx} | pred shape: {pred_input.shape}, target shape: {target_input.shape} | Error: {str(e)}\")\n",
    "            hd = float('nan')\n",
    "\n",
    "        dice_all.append(torch.tensor(mean_dice))\n",
    "        iou_all.append(torch.tensor(mean_iou))\n",
    "        hd_all.append(torch.tensor(hd))\n",
    "\n",
    "# ✅ Final average results\n",
    "skipped_hd = torch.isnan(torch.tensor(hd_all)).sum().item()\n",
    "print(\"\\n📊 Test Results:\")\n",
    "print(f\"Avg Dice Score: {torch.mean(torch.stack(dice_all)).item():.4f}\")\n",
    "print(f\"Avg IOU Score: {torch.mean(torch.stack(iou_all)).item():.4f}\")\n",
    "print(f\"Avg Hausdorff Distance: {torch.nanmean(torch.stack(hd_all)).item():.4f}\")\n",
    "print(f\"ℹ️ Skipped Hausdorff for {int(skipped_hd)} samples due to errors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f95f876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::mul encountered 24 time(s)\n",
      "Unsupported operator aten::add encountered 24 time(s)\n",
      "Unsupported operator aten::softmax encountered 8 time(s)\n",
      "Unsupported operator aten::gelu encountered 15 time(s)\n",
      "Unsupported operator aten::upsample_trilinear3d encountered 4 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GFLOPs: 203.12 G\n",
      "✅ Parameter Count:\n",
      "| name                         | #elements or shape   |\n",
      "|:-----------------------------|:---------------------|\n",
      "| model                        | 15.7M                |\n",
      "|  encoder                     |  15.3M               |\n",
      "|   encoder.stages             |   9.5M               |\n",
      "|    encoder.stages.0          |    0.1M              |\n",
      "|    encoder.stages.1          |    0.4M              |\n",
      "|    encoder.stages.2          |    2.5M              |\n",
      "|    encoder.stages.3          |    6.4M              |\n",
      "|   encoder.embeds             |   5.8M               |\n",
      "|    encoder.embeds.0          |    7.1K              |\n",
      "|    encoder.embeds.1          |    0.2M              |\n",
      "|    encoder.embeds.2          |    1.1M              |\n",
      "|    encoder.embeds.3          |    4.4M              |\n",
      "|   encoder.norms              |   2.0K               |\n",
      "|    encoder.norms.0           |    0.1K              |\n",
      "|    encoder.norms.1           |    0.3K              |\n",
      "|    encoder.norms.2           |    0.6K              |\n",
      "|    encoder.norms.3           |    1.0K              |\n",
      "|  decoder                     |  0.4M                |\n",
      "|   decoder.decode3            |   0.3M               |\n",
      "|    decoder.decode3.attn_fuse |    0.2M              |\n",
      "|    decoder.decode3.conv      |    73.5K             |\n",
      "|   decoder.decode2            |   69.9K              |\n",
      "|    decoder.decode2.attn_fuse |    49.5K             |\n",
      "|    decoder.decode2.conv      |    20.4K             |\n",
      "|   decoder.decode1            |   18.6K              |\n",
      "|    decoder.decode1.attn_fuse |    12.5K             |\n",
      "|    decoder.decode1.conv      |    6.1K              |\n",
      "|   decoder.final_up           |   55.4K              |\n",
      "|    decoder.final_up.1        |    55.3K             |\n",
      "|    decoder.final_up.2        |    64                |\n",
      "|   decoder.seg_head           |   99                 |\n",
      "|    decoder.seg_head.weight   |    (3, 32, 1, 1, 1)  |\n",
      "|    decoder.seg_head.bias     |    (3,)              |\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
    "from model4 import RefineFormer3D\n",
    "from config import IN_CHANNELS, NUM_CLASSES\n",
    "\n",
    "# Set device to CPU for analysis\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "model = RefineFormer3D(in_channels=IN_CHANNELS, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Dummy 3D input: [Batch, Channels, Depth, Height, Width]\n",
    "dummy_input = torch.randn(1, IN_CHANNELS, 128, 128, 128).to(DEVICE)\n",
    "\n",
    "# Compute FLOPs\n",
    "with torch.no_grad():\n",
    "    flops = FlopCountAnalysis(model, dummy_input)\n",
    "    flops_result = flops.total() / 1e9  # Convert to GFLOPs\n",
    "\n",
    "# Print results\n",
    "print(f\"✅ GFLOPs: {flops_result:.2f} G\")\n",
    "print(\"✅ Parameter Count:\")\n",
    "print(parameter_count_table(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b2e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model4 import RefineFormer3D\n",
    "from thop import profile\n",
    "from torchinfo import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model with your config\n",
    "model = RefineFormer3D(\n",
    "    in_channels=4,\n",
    "    num_classes=3,\n",
    "    embed_dims=[64, 128, 320, 512],\n",
    "    depths=[2, 2, 2, 2],\n",
    "    num_heads=[1, 2, 4, 8],\n",
    "    window_sizes=[(4, 4, 4), (2, 4, 4), (2, 2, 2), (1, 2, 2)],\n",
    "    mlp_ratios=[4, 4, 4, 4],\n",
    "    decoder_channels=[256, 128, 64, 32]\n",
    ").to(device)\n",
    "\n",
    "# Dummy input\n",
    "dummy_input = torch.randn(1, 4, 128, 128, 128).to(device)\n",
    "\n",
    "# Check output shape\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "    print(\"✅ Output shape:\", output[\"main\"].shape)\n",
    "\n",
    "# Param count\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"📊 Total Parameters: {total_params / 1e6:.2f}M\")\n",
    "\n",
    "# THOP for GFLOPs\n",
    "macs, params = profile(model, inputs=(dummy_input,), verbose=False)\n",
    "print(f\"⚙️ Estimated MACs: {macs / 1e9:.2f} GFLOPs\")\n",
    "\n",
    "# Optional: torchinfo summary\n",
    "summary(model, input_size=(1, 4, 128, 128, 128), depth=4, device=device.type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b6575cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Missing segmentation files:\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_195_1/Brats17_TCIA_195_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_AAM_1/Brats17_CBICA_AAM_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ABT_1/Brats17_CBICA_ABT_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ALA_1/Brats17_CBICA_ALA_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ALT_1/Brats17_CBICA_ALT_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ALV_1/Brats17_CBICA_ALV_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ALZ_1/Brats17_CBICA_ALZ_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_AMF_1/Brats17_CBICA_AMF_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_AMU_1/Brats17_CBICA_AMU_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ANK_1/Brats17_CBICA_ANK_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_APM_1/Brats17_CBICA_APM_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_AQE_1/Brats17_CBICA_AQE_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ARR_1/Brats17_CBICA_ARR_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ATW_1/Brats17_CBICA_ATW_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_AUC_1/Brats17_CBICA_AUC_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_AUE_1/Brats17_CBICA_AUE_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_AZA_1/Brats17_CBICA_AZA_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_212_1/Brats17_TCIA_212_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_216_1/Brats17_TCIA_216_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_230_1/Brats17_TCIA_230_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_248_1/Brats17_TCIA_248_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_253_1/Brats17_TCIA_253_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_288_1/Brats17_TCIA_288_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_311_1/Brats17_TCIA_311_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_313_1/Brats17_TCIA_313_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_400_1/Brats17_TCIA_400_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_600_1/Brats17_TCIA_600_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_601_1/Brats17_TCIA_601_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_602_1/Brats17_TCIA_602_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_604_1/Brats17_TCIA_604_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_609_1/Brats17_TCIA_609_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_610_1/Brats17_TCIA_610_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_611_1/Brats17_TCIA_611_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_612_1/Brats17_TCIA_612_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_613_1/Brats17_TCIA_613_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_617_1/Brats17_TCIA_617_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_636_1/Brats17_TCIA_636_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_638_1/Brats17_TCIA_638_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_646_1/Brats17_TCIA_646_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_652_1/Brats17_TCIA_652_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_UAB_3446_1/Brats17_UAB_3446_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_UAB_3454_1/Brats17_UAB_3454_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_UAB_3455_1/Brats17_UAB_3455_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_UAB_3456_1/Brats17_UAB_3456_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_UAB_3498_1/Brats17_UAB_3498_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_UAB_3499_1/Brats17_UAB_3499_1_seg.nii\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def find_missing_seg_files(validation_dir):\n",
    "    missing = []\n",
    "    for case in os.listdir(validation_dir):\n",
    "        case_path = os.path.join(validation_dir, case)\n",
    "        if os.path.isdir(case_path):\n",
    "            seg_file = os.path.join(case_path, f\"{case}_seg.nii\")\n",
    "            if not os.path.isfile(seg_file):\n",
    "                missing.append(seg_file)\n",
    "    return missing\n",
    "\n",
    "# Example usage\n",
    "validation_dir = \"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData\"\n",
    "missing_files = find_missing_seg_files(validation_dir)\n",
    "\n",
    "if missing_files:\n",
    "    print(\"⚠️ Missing segmentation files:\")\n",
    "    for f in missing_files:\n",
    "        print(f)\n",
    "else:\n",
    "    print(\"✅ All segmentation files are present.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2202c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def check_missing_seg(root_dir):\n",
    "    missing_patients = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for dirname in dirnames:\n",
    "            seg_path = os.path.join(dirpath, dirname, f\"{dirname}_seg.nii\")\n",
    "            if not os.path.exists(seg_path):\n",
    "                missing_patients.append(dirname)\n",
    "    return missing_patients\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = \"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17TrainingData/HGG/\"\n",
    "    missing_hgg = check_missing_seg(data_dir)\n",
    "    \n",
    "    data_dir = \"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17TrainingData/LGG/\"\n",
    "    missing_lgg = check_missing_seg(data_dir)\n",
    "\n",
    "    print(\"Missing in HGG:\", missing_hgg)\n",
    "    print(\"Missing in LGG:\", missing_lgg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b678a36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔵 Checking environment:\n",
      "\n",
      "Torch Version        : 2.4.1+cu118\n",
      "CUDA Version         : 11.8\n",
      "cuDNN Version        : 90100\n",
      "GPU Available        : True\n",
      "Device Count         : 1\n",
      "Current Device       : 0\n",
      "Device Name          : NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "\n",
      "🟡 Checking autocast compatibility:\n",
      "❌ autocast(device_type='cuda') not supported: __init__() got an unexpected keyword argument 'device_type'\n",
      "✅ You must use: autocast() only.\n",
      "\n",
      "🟡 Checking GradScaler compatibility:\n",
      "✅ GradScaler() works (no device_type needed).\n",
      "\n",
      "✅ Environment check complete.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3847/3946993505.py:33: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
      "/tmp/ipykernel_3847/3946993505.py:42: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_environment():\n",
    "    print(\"\\n🔵 Checking environment:\\n\")\n",
    "\n",
    "    # PyTorch Version\n",
    "    print(f\"Torch Version        : {torch.__version__}\")\n",
    "\n",
    "    # CUDA Version\n",
    "    if torch.version.cuda:\n",
    "        print(f\"CUDA Version         : {torch.version.cuda}\")\n",
    "    else:\n",
    "        print(\"CUDA Version         : None (CPU Only)\")\n",
    "\n",
    "    # cuDNN Version\n",
    "    if torch.backends.cudnn.is_available():\n",
    "        print(f\"cuDNN Version        : {torch.backends.cudnn.version()}\")\n",
    "    else:\n",
    "        print(\"cuDNN Version        : None\")\n",
    "\n",
    "    # GPU Availability\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    print(f\"GPU Available        : {gpu_available}\")\n",
    "\n",
    "    if gpu_available:\n",
    "        print(f\"Device Count         : {torch.cuda.device_count()}\")\n",
    "        print(f\"Current Device       : {torch.cuda.current_device()}\")\n",
    "        print(f\"Device Name          : {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    print(\"\\n🟡 Checking autocast compatibility:\")\n",
    "\n",
    "    try:\n",
    "        with torch.cuda.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            print(\"✅ autocast(device_type='cuda', dtype=torch.float16) works!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ autocast(device_type='cuda') not supported: {e}\")\n",
    "        print(\"✅ You must use: autocast() only.\")\n",
    "\n",
    "    print(\"\\n🟡 Checking GradScaler compatibility:\")\n",
    "\n",
    "    try:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        print(\"✅ GradScaler() works (no device_type needed).\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ GradScaler() failed: {e}\")\n",
    "\n",
    "    print(\"\\n✅ Environment check complete.\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_environment()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
