{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import RefineFormer3D\n",
    "from losses import RefineFormer3DLoss\n",
    "from optimizer import get_optimizer, get_scheduler\n",
    "from metrics import dice_coefficient, iou_score, precision_recall_f1, hausdorff_distance\n",
    "from dataset import BraTSDataset\n",
    "from augmentation import Compose3D, RandomFlip3D, RandomRotation3D, RandomNoise3D\n",
    "from config import DEVICE, IN_CHANNELS, NUM_CLASSES, BASE_LR, WEIGHT_DECAY, NUM_EPOCHS\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, scaler):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, targets in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # Mixed precision\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    return epoch_loss\n",
    "\n",
    "def validate_one_epoch(model, dataloader, criterion, device, num_classes):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    dice_all, iou_all, hausdorff_all = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader, desc=\"Validation\"):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            with autocast():  # Optional mixed precision inference\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "            preds = outputs[\"main\"]\n",
    "\n",
    "            dice_scores, _ = dice_coefficient(preds, targets, num_classes)\n",
    "            iou_scores, _ = iou_score(preds, targets, num_classes)\n",
    "            hausdorff = hausdorff_distance(preds, targets)\n",
    "\n",
    "            dice_all.append(np.mean(dice_scores))\n",
    "            iou_all.append(np.mean(iou_scores))\n",
    "            hausdorff_all.append(hausdorff)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    mean_dice = np.nanmean(dice_all)\n",
    "    mean_iou = np.nanmean(iou_all)\n",
    "    mean_hausdorff = np.nanmean(hausdorff_all)\n",
    "\n",
    "    return epoch_loss, mean_dice, mean_iou, mean_hausdorff\n",
    "\n",
    "def save_checkpoint(state, save_dir, filename=\"best_model.pth\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save(state, os.path.join(save_dir, filename))\n",
    "\n",
    "def main():\n",
    "    # ==================== CONFIG ====================\n",
    "    device = torch.device(DEVICE)\n",
    "    save_dir = \"./checkpoints\"\n",
    "    best_dice = 0.0\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # ==================== MODEL ====================\n",
    "    model = RefineFormer3D(in_channels=IN_CHANNELS, num_classes=NUM_CLASSES)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # ==================== OPTIMIZER + LOSS ====================\n",
    "    optimizer = get_optimizer(model, base_lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = get_scheduler(optimizer, mode=\"cosine\", T_max=NUM_EPOCHS)\n",
    "    criterion = RefineFormer3DLoss()\n",
    "\n",
    "    # ==================== DATASET + DATALOADER ====================\n",
    "    train_transform = Compose3D([\n",
    "        RandomFlip3D(p=0.5),\n",
    "        RandomRotation3D(p=0.5),\n",
    "        RandomNoise3D(p=0.3),\n",
    "    ])\n",
    "\n",
    "    train_dataset = BraTSDataset(\n",
    "    root_dirs=[\n",
    "        \"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17TrainingData/HGG\",   # 🔥 Replace this\n",
    "        \"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17TrainingData/LGG\"    # 🔥 Replace this\n",
    "    ],\n",
    "    transform=train_transform,\n",
    "    )   \n",
    "\n",
    "    val_dataset = BraTSDataset(\n",
    "    root_dirs=[\n",
    "        \"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData\"      # 🔥 Replace this\n",
    "    ],\n",
    "    transform=None,\n",
    "    )\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # ==================== TRAINING LOOP ====================\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        print(f\"\\n--- Epoch [{epoch}/{NUM_EPOCHS}] ---\")\n",
    "\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, scaler)\n",
    "        val_loss, val_dice, val_iou, val_hd = validate_one_epoch(model, val_loader, criterion, device, NUM_CLASSES)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Dice: {val_dice:.4f} | Val IoU: {val_iou:.4f} | Hausdorff: {val_hd:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_dice > best_dice:\n",
    "            best_dice = val_dice\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_dice': best_dice,\n",
    "            }, save_dir)\n",
    "            print(f\"✅ Saved Best Model (Dice: {best_dice:.4f})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3164226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b6e7fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35015/2899409100.py:88: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # ✅ Fixed warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch [1/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/284 [00:00<?, ?it/s]/tmp/ipykernel_35015/2899409100.py:30: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():   # ✅ Specify dtype explicitly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 targets dtype: torch.int64\n",
      "🧪 targets unique: tensor([0, 1, 2], device='cuda:0')\n",
      "🧪 targets shape: torch.Size([1, 128, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 2/284 [00:15<31:04,  6.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 targets dtype: torch.int64\n",
      "🧪 targets unique: tensor([0, 1, 2], device='cuda:0')\n",
      "🧪 targets shape: torch.Size([1, 128, 128, 128])\n",
      "🧪 targets dtype: torch.int64\n",
      "🧪 targets unique: tensor([0, 1, 2], device='cuda:0')\n",
      "🧪 targets shape: torch.Size([1, 128, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏         | 4/284 [00:16<10:34,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 targets dtype: torch.int64\n",
      "🧪 targets unique: tensor([0, 1, 2], device='cuda:0')\n",
      "🧪 targets shape: torch.Size([1, 128, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 5/284 [00:30<30:17,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 targets dtype: torch.int64\n",
      "🧪 targets unique: tensor([0, 1, 2], device='cuda:0')\n",
      "🧪 targets shape: torch.Size([1, 128, 128, 128])\n",
      "🧪 targets dtype: torch.int64\n",
      "🧪 targets unique: tensor([0, 1, 2], device='cuda:0')\n",
      "🧪 targets shape: torch.Size([1, 128, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 7/284 [00:30<13:43,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 targets dtype: torch.int64\n",
      "🧪 targets unique: tensor([0, 1, 2], device='cuda:0')\n",
      "🧪 targets shape: torch.Size([1, 128, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 129\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal Dice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_dice\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val IOU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_iou\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val Hausdorff: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_hd\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 129\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 119\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, NUM_EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Epoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 119\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     val_loss, val_dice, val_iou, val_hd \u001b[38;5;241m=\u001b[39m validate_one_epoch(model, val_loader, criterion, device, NUM_CLASSES)\n\u001b[1;32m    122\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, device, scaler)\u001b[0m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     21\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     24\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1283\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1283\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1285\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import RefineFormer3D\n",
    "from dataset import BraTSDataset\n",
    "from optimizer import get_optimizer, get_scheduler\n",
    "from metrics import dice_coefficient, iou_score, hausdorff_distance\n",
    "from augmentation import Compose3D, RandomFlip3D, RandomRotation3D, RandomNoise3D\n",
    "from losses import RefineFormer3DLoss   # ✅ Correct loss import\n",
    "\n",
    "from config import DEVICE, IN_CHANNELS, NUM_CLASSES, BASE_LR, WEIGHT_DECAY, NUM_EPOCHS\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, scaler):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, targets in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device=device, dtype=torch.long, non_blocking=True)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():   # ✅ Specify dtype explicitly\n",
    "            outputs = model(inputs)['main']\n",
    "            print(\"🧪 targets dtype:\", targets.dtype)\n",
    "            print(\"🧪 targets unique:\", torch.unique(targets))\n",
    "            print(\"🧪 targets shape:\", targets.shape)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, dataloader, criterion, device, num_classes):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    dice_all, iou_all, hd_all = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device=device, dtype=torch.long, non_blocking=True)\n",
    "\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets) \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            dice_all.append(dice_coefficient(preds, targets, num_classes))\n",
    "            iou_all.append(iou_score(preds, targets, num_classes))\n",
    "            hd_all.append(hausdorff_distance(preds, targets))\n",
    "\n",
    "    val_loss = running_loss / len(dataloader.dataset)\n",
    "    avg_dice = torch.mean(torch.stack(dice_all)).item()\n",
    "    avg_iou = torch.mean(torch.stack(iou_all)).item()\n",
    "    avg_hd = torch.mean(torch.stack(hd_all)).item()\n",
    "\n",
    "    return val_loss, avg_dice, avg_iou, avg_hd\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = DEVICE\n",
    "\n",
    "    # Model\n",
    "    model = RefineFormer3D(in_channels=IN_CHANNELS, num_classes=NUM_CLASSES)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Optimizer, Scheduler, Loss\n",
    "    optimizer = get_optimizer(model, base_lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "    criterion = RefineFormer3DLoss()   # ✅ Correct\n",
    "    scaler = GradScaler()  # ✅ Fixed warning\n",
    "\n",
    "    # Data\n",
    "    train_transform = Compose3D([\n",
    "        RandomFlip3D(p=0.5),\n",
    "        RandomRotation3D(p=0.5),\n",
    "        RandomNoise3D(p=0.3),\n",
    "    ])\n",
    "\n",
    "    train_dataset = BraTSDataset(\n",
    "        root_dirs=[\n",
    "            \"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17TrainingData/HGG\",\n",
    "            \"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17TrainingData/LGG\",\n",
    "        ],\n",
    "        transform=train_transform,\n",
    "    )\n",
    "\n",
    "    val_dataset = BraTSDataset(\n",
    "        root_dirs=[\n",
    "            \"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData\",\n",
    "        ],\n",
    "        transform=None,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        print(f\"\\n--- Epoch [{epoch}/{NUM_EPOCHS}] ---\")\n",
    "\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, scaler)\n",
    "        val_loss, val_dice, val_iou, val_hd = validate_one_epoch(model, val_loader, criterion, device, NUM_CLASSES)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}  |  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Val Dice: {val_dice:.4f} | Val IOU: {val_iou:.4f} | Val Hausdorff: {val_hd:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b6575cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Missing segmentation files:\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_195_1/Brats17_TCIA_195_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_AAM_1/Brats17_CBICA_AAM_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ABT_1/Brats17_CBICA_ABT_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ALA_1/Brats17_CBICA_ALA_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ALT_1/Brats17_CBICA_ALT_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ALV_1/Brats17_CBICA_ALV_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ALZ_1/Brats17_CBICA_ALZ_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_AMF_1/Brats17_CBICA_AMF_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_AMU_1/Brats17_CBICA_AMU_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ANK_1/Brats17_CBICA_ANK_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_APM_1/Brats17_CBICA_APM_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_AQE_1/Brats17_CBICA_AQE_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ARR_1/Brats17_CBICA_ARR_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ATW_1/Brats17_CBICA_ATW_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_AUC_1/Brats17_CBICA_AUC_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_AUE_1/Brats17_CBICA_AUE_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_AZA_1/Brats17_CBICA_AZA_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_212_1/Brats17_TCIA_212_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_216_1/Brats17_TCIA_216_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_230_1/Brats17_TCIA_230_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_248_1/Brats17_TCIA_248_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_253_1/Brats17_TCIA_253_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_288_1/Brats17_TCIA_288_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_311_1/Brats17_TCIA_311_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_313_1/Brats17_TCIA_313_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_400_1/Brats17_TCIA_400_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_600_1/Brats17_TCIA_600_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_601_1/Brats17_TCIA_601_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_602_1/Brats17_TCIA_602_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_604_1/Brats17_TCIA_604_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_609_1/Brats17_TCIA_609_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_610_1/Brats17_TCIA_610_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_611_1/Brats17_TCIA_611_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_612_1/Brats17_TCIA_612_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_613_1/Brats17_TCIA_613_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_617_1/Brats17_TCIA_617_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_636_1/Brats17_TCIA_636_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_638_1/Brats17_TCIA_638_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_646_1/Brats17_TCIA_646_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_652_1/Brats17_TCIA_652_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_UAB_3446_1/Brats17_UAB_3446_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_UAB_3454_1/Brats17_UAB_3454_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_UAB_3455_1/Brats17_UAB_3455_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_UAB_3456_1/Brats17_UAB_3456_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_UAB_3498_1/Brats17_UAB_3498_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_UAB_3499_1/Brats17_UAB_3499_1_seg.nii\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def find_missing_seg_files(validation_dir):\n",
    "    missing = []\n",
    "    for case in os.listdir(validation_dir):\n",
    "        case_path = os.path.join(validation_dir, case)\n",
    "        if os.path.isdir(case_path):\n",
    "            seg_file = os.path.join(case_path, f\"{case}_seg.nii\")\n",
    "            if not os.path.isfile(seg_file):\n",
    "                missing.append(seg_file)\n",
    "    return missing\n",
    "\n",
    "# Example usage\n",
    "validation_dir = \"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData\"\n",
    "missing_files = find_missing_seg_files(validation_dir)\n",
    "\n",
    "if missing_files:\n",
    "    print(\"⚠️ Missing segmentation files:\")\n",
    "    for f in missing_files:\n",
    "        print(f)\n",
    "else:\n",
    "    print(\"✅ All segmentation files are present.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2202c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def check_missing_seg(root_dir):\n",
    "    missing_patients = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for dirname in dirnames:\n",
    "            seg_path = os.path.join(dirpath, dirname, f\"{dirname}_seg.nii\")\n",
    "            if not os.path.exists(seg_path):\n",
    "                missing_patients.append(dirname)\n",
    "    return missing_patients\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = \"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17TrainingData/HGG/\"\n",
    "    missing_hgg = check_missing_seg(data_dir)\n",
    "    \n",
    "    data_dir = \"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17TrainingData/LGG/\"\n",
    "    missing_lgg = check_missing_seg(data_dir)\n",
    "\n",
    "    print(\"Missing in HGG:\", missing_hgg)\n",
    "    print(\"Missing in LGG:\", missing_lgg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def check_environment():\n",
    "    print(\"\\n🔵 Checking environment:\\n\")\n",
    "\n",
    "    # PyTorch Version\n",
    "    print(f\"Torch Version        : {torch.__version__}\")\n",
    "\n",
    "    # CUDA Version\n",
    "    if torch.version.cuda:\n",
    "        print(f\"CUDA Version         : {torch.version.cuda}\")\n",
    "    else:\n",
    "        print(\"CUDA Version         : None (CPU Only)\")\n",
    "\n",
    "    # cuDNN Version\n",
    "    if torch.backends.cudnn.is_available():\n",
    "        print(f\"cuDNN Version        : {torch.backends.cudnn.version()}\")\n",
    "    else:\n",
    "        print(\"cuDNN Version        : None\")\n",
    "\n",
    "    # GPU Availability\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    print(f\"GPU Available        : {gpu_available}\")\n",
    "\n",
    "    if gpu_available:\n",
    "        print(f\"Device Count         : {torch.cuda.device_count()}\")\n",
    "        print(f\"Current Device       : {torch.cuda.current_device()}\")\n",
    "        print(f\"Device Name          : {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    print(\"\\n🟡 Checking autocast compatibility:\")\n",
    "\n",
    "    try:\n",
    "        with torch.cuda.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            print(\"✅ autocast(device_type='cuda', dtype=torch.float16) works!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ autocast(device_type='cuda') not supported: {e}\")\n",
    "        print(\"✅ You must use: autocast() only.\")\n",
    "\n",
    "    print(\"\\n🟡 Checking GradScaler compatibility:\")\n",
    "\n",
    "    try:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        print(\"✅ GradScaler() works (no device_type needed).\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ GradScaler() failed: {e}\")\n",
    "\n",
    "    print(\"\\n✅ Environment check complete.\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_environment()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
