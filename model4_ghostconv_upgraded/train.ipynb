{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd1827a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 90\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Final model saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 79\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, NUM_EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Epoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, device, scaler)\u001b[0m\n\u001b[1;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     28\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     31\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m pad_input_for_windows(inputs, window_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1283\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1283\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1285\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "from model5_upgrade import RefineFormer3D\n",
    "from dataset import BraTSDataset\n",
    "from optimizer import get_optimizer, get_scheduler\n",
    "from augmentation import Compose3D, RandomFlip3D, RandomRotation3D, RandomNoise3D\n",
    "from losses import RefineFormer3DLoss\n",
    "import torch.nn.functional as F\n",
    "from config import DEVICE, IN_CHANNELS, NUM_CLASSES, BASE_LR, WEIGHT_DECAY, NUM_EPOCHS\n",
    "\n",
    "def pad_input_for_windows(x, window_size=(2, 2, 2)):\n",
    "    # x: [B, C, D, H, W]\n",
    "    _, _, D, H, W = x.shape\n",
    "    pad_d = (window_size[0] - D % window_size[0]) % window_size[0]\n",
    "    pad_h = (window_size[1] - H % window_size[1]) % window_size[1]\n",
    "    pad_w = (window_size[2] - W % window_size[2]) % window_size[2]\n",
    "\n",
    "    return F.pad(x, (0, pad_w, 0, pad_h, 0, pad_d))  # (W_left, W_right, H_top, H_bottom, D_front, D_back)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, scaler):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, targets in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        inputs = pad_input_for_windows(inputs, window_size=(2, 2, 2))\n",
    "\n",
    "        targets = targets.to(device=device, dtype=torch.long, non_blocking=True)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(inputs)\n",
    "            if torch.isnan(outputs[\"main\"]).any():\n",
    "                print(\" Model output contains NaNs \")\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = DEVICE\n",
    "    model = RefineFormer3D(in_channels=IN_CHANNELS, num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "    optimizer = get_optimizer(model, base_lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "    criterion = RefineFormer3DLoss()\n",
    "    scaler = GradScaler(device='cuda')\n",
    "\n",
    "    train_transform = Compose3D([\n",
    "        RandomFlip3D(p=0.5),\n",
    "        RandomRotation3D(p=0.5),\n",
    "        RandomNoise3D(p=0.3),\n",
    "    ])    \n",
    "\n",
    "    train_dataset = BraTSDataset(\n",
    "        root_dirs=[\"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BRATS_SPLIT/train\"],\n",
    "        transform=train_transform,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=14, pin_memory=True)\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        print(f\"\\n--- Epoch [{epoch}/{NUM_EPOCHS}] ---\")\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, scaler)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        torch.save(model.state_dict(), f\"checkpoint_epoch_{epoch}.pt\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"final_model.pt\")\n",
    "    print(\" Final model saved as 'final_model.pt'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d57d02",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mepoch\u001b[49m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: scaler\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m      6\u001b[0m }, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch' is not defined"
     ]
    }
   ],
   "source": [
    "epoch = 90  # or whatever epoch you just finished\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scaler_state_dict': scaler.state_dict(),\n",
    "}, f\"checkpoint_epoch_{epoch}.pt\")\n",
    "\n",
    "print(f\"✅ Saved checkpoint for epoch {epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2696f67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total valid patient directories found: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152291/563205197.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"checkpoint_epoch_124.pt\", map_location=DEVICE))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for RefineFormer3D:\n\tMissing key(s) in state_dict: \"patch_embed.proj.primary_conv.weight\", \"patch_embed.proj.cheap_operation.weight\", \"patch_embed.proj.se.fc.0.weight\", \"patch_embed.proj.se.fc.0.bias\", \"patch_embed.proj.se.fc.2.weight\", \"patch_embed.proj.se.fc.2.bias\", \"patch_embed.dw.weight\", \"patch_embed.dw.bias\", \"patch_embed.norm.weight\", \"patch_embed.norm.bias\", \"blocks.0.norm.weight\", \"blocks.0.norm.bias\", \"blocks.0.attn.depth.in_proj_weight\", \"blocks.0.attn.depth.in_proj_bias\", \"blocks.0.attn.depth.out_proj.weight\", \"blocks.0.attn.depth.out_proj.bias\", \"blocks.0.attn.height.in_proj_weight\", \"blocks.0.attn.height.in_proj_bias\", \"blocks.0.attn.height.out_proj.weight\", \"blocks.0.attn.height.out_proj.bias\", \"blocks.0.attn.width.in_proj_weight\", \"blocks.0.attn.width.in_proj_bias\", \"blocks.0.attn.width.out_proj.weight\", \"blocks.0.attn.width.out_proj.bias\", \"blocks.1.norm.weight\", \"blocks.1.norm.bias\", \"blocks.1.attn.depth.in_proj_weight\", \"blocks.1.attn.depth.in_proj_bias\", \"blocks.1.attn.depth.out_proj.weight\", \"blocks.1.attn.depth.out_proj.bias\", \"blocks.1.attn.height.in_proj_weight\", \"blocks.1.attn.height.in_proj_bias\", \"blocks.1.attn.height.out_proj.weight\", \"blocks.1.attn.height.out_proj.bias\", \"blocks.1.attn.width.in_proj_weight\", \"blocks.1.attn.width.in_proj_bias\", \"blocks.1.attn.width.out_proj.weight\", \"blocks.1.attn.width.out_proj.bias\", \"blocks.2.norm.weight\", \"blocks.2.norm.bias\", \"blocks.2.attn.depth.in_proj_weight\", \"blocks.2.attn.depth.in_proj_bias\", \"blocks.2.attn.depth.out_proj.weight\", \"blocks.2.attn.depth.out_proj.bias\", \"blocks.2.attn.height.in_proj_weight\", \"blocks.2.attn.height.in_proj_bias\", \"blocks.2.attn.height.out_proj.weight\", \"blocks.2.attn.height.out_proj.bias\", \"blocks.2.attn.width.in_proj_weight\", \"blocks.2.attn.width.in_proj_bias\", \"blocks.2.attn.width.out_proj.weight\", \"blocks.2.attn.width.out_proj.bias\", \"blocks.3.norm.weight\", \"blocks.3.norm.bias\", \"blocks.3.attn.depth.in_proj_weight\", \"blocks.3.attn.depth.in_proj_bias\", \"blocks.3.attn.depth.out_proj.weight\", \"blocks.3.attn.depth.out_proj.bias\", \"blocks.3.attn.height.in_proj_weight\", \"blocks.3.attn.height.in_proj_bias\", \"blocks.3.attn.height.out_proj.weight\", \"blocks.3.attn.height.out_proj.bias\", \"blocks.3.attn.width.in_proj_weight\", \"blocks.3.attn.width.in_proj_bias\", \"blocks.3.attn.width.out_proj.weight\", \"blocks.3.attn.width.out_proj.bias\", \"blocks.4.norm.weight\", \"blocks.4.norm.bias\", \"blocks.4.attn.depth.in_proj_weight\", \"blocks.4.attn.depth.in_proj_bias\", \"blocks.4.attn.depth.out_proj.weight\", \"blocks.4.attn.depth.out_proj.bias\", \"blocks.4.attn.height.in_proj_weight\", \"blocks.4.attn.height.in_proj_bias\", \"blocks.4.attn.height.out_proj.weight\", \"blocks.4.attn.height.out_proj.bias\", \"blocks.4.attn.width.in_proj_weight\", \"blocks.4.attn.width.in_proj_bias\", \"blocks.4.attn.width.out_proj.weight\", \"blocks.4.attn.width.out_proj.bias\", \"blocks.5.norm.weight\", \"blocks.5.norm.bias\", \"blocks.5.attn.depth.in_proj_weight\", \"blocks.5.attn.depth.in_proj_bias\", \"blocks.5.attn.depth.out_proj.weight\", \"blocks.5.attn.depth.out_proj.bias\", \"blocks.5.attn.height.in_proj_weight\", \"blocks.5.attn.height.in_proj_bias\", \"blocks.5.attn.height.out_proj.weight\", \"blocks.5.attn.height.out_proj.bias\", \"blocks.5.attn.width.in_proj_weight\", \"blocks.5.attn.width.in_proj_bias\", \"blocks.5.attn.width.out_proj.weight\", \"blocks.5.attn.width.out_proj.bias\", \"decoder.conv.primary_conv.weight\", \"decoder.conv.cheap_operation.weight\", \"decoder.conv.se.fc.0.weight\", \"decoder.conv.se.fc.0.bias\", \"decoder.conv.se.fc.2.weight\", \"decoder.conv.se.fc.2.bias\", \"decoder.norm.weight\", \"decoder.norm.bias\", \"decoder.head.weight\", \"decoder.head.bias\", \"aux_head.weight\", \"aux_head.bias\", \"boundary_head.conv.weight\", \"boundary_head.conv.bias\". \n\tUnexpected key(s) in state_dict: \"encoder.embeds.0.proj.primary_conv.weight\", \"encoder.embeds.0.proj.cheap_operation.weight\", \"encoder.embeds.0.norm.weight\", \"encoder.embeds.0.norm.bias\", \"encoder.embeds.1.proj.primary_conv.weight\", \"encoder.embeds.1.proj.cheap_operation.weight\", \"encoder.embeds.1.norm.weight\", \"encoder.embeds.1.norm.bias\", \"encoder.embeds.2.proj.primary_conv.weight\", \"encoder.embeds.2.proj.cheap_operation.weight\", \"encoder.embeds.2.norm.weight\", \"encoder.embeds.2.norm.bias\", \"encoder.embeds.3.proj.primary_conv.weight\", \"encoder.embeds.3.proj.cheap_operation.weight\", \"encoder.embeds.3.norm.weight\", \"encoder.embeds.3.norm.bias\", \"encoder.stages.0.0.norm1.weight\", \"encoder.stages.0.0.norm1.bias\", \"encoder.stages.0.0.attn.relative_bias\", \"encoder.stages.0.0.attn.rel_idx\", \"encoder.stages.0.0.attn.qkv.weight\", \"encoder.stages.0.0.attn.qkv.bias\", \"encoder.stages.0.0.attn.proj.weight\", \"encoder.stages.0.0.attn.proj.bias\", \"encoder.stages.0.0.norm2.weight\", \"encoder.stages.0.0.norm2.bias\", \"encoder.stages.0.0.mlp.fc1.weight\", \"encoder.stages.0.0.mlp.fc1.bias\", \"encoder.stages.0.0.mlp.dwconv.weight\", \"encoder.stages.0.0.mlp.dwconv.bias\", \"encoder.stages.0.0.mlp.fc2.weight\", \"encoder.stages.0.0.mlp.fc2.bias\", \"encoder.stages.0.1.norm1.weight\", \"encoder.stages.0.1.norm1.bias\", \"encoder.stages.0.1.attn.relative_bias\", \"encoder.stages.0.1.attn.rel_idx\", \"encoder.stages.0.1.attn.qkv.weight\", \"encoder.stages.0.1.attn.qkv.bias\", \"encoder.stages.0.1.attn.proj.weight\", \"encoder.stages.0.1.attn.proj.bias\", \"encoder.stages.0.1.norm2.weight\", \"encoder.stages.0.1.norm2.bias\", \"encoder.stages.0.1.mlp.fc1.weight\", \"encoder.stages.0.1.mlp.fc1.bias\", \"encoder.stages.0.1.mlp.dwconv.weight\", \"encoder.stages.0.1.mlp.dwconv.bias\", \"encoder.stages.0.1.mlp.fc2.weight\", \"encoder.stages.0.1.mlp.fc2.bias\", \"encoder.stages.1.0.norm1.weight\", \"encoder.stages.1.0.norm1.bias\", \"encoder.stages.1.0.attn.relative_bias\", \"encoder.stages.1.0.attn.rel_idx\", \"encoder.stages.1.0.attn.qkv.weight\", \"encoder.stages.1.0.attn.qkv.bias\", \"encoder.stages.1.0.attn.proj.weight\", \"encoder.stages.1.0.attn.proj.bias\", \"encoder.stages.1.0.norm2.weight\", \"encoder.stages.1.0.norm2.bias\", \"encoder.stages.1.0.mlp.fc1.weight\", \"encoder.stages.1.0.mlp.fc1.bias\", \"encoder.stages.1.0.mlp.dwconv.weight\", \"encoder.stages.1.0.mlp.dwconv.bias\", \"encoder.stages.1.0.mlp.fc2.weight\", \"encoder.stages.1.0.mlp.fc2.bias\", \"encoder.stages.1.1.norm1.weight\", \"encoder.stages.1.1.norm1.bias\", \"encoder.stages.1.1.attn.relative_bias\", \"encoder.stages.1.1.attn.rel_idx\", \"encoder.stages.1.1.attn.qkv.weight\", \"encoder.stages.1.1.attn.qkv.bias\", \"encoder.stages.1.1.attn.proj.weight\", \"encoder.stages.1.1.attn.proj.bias\", \"encoder.stages.1.1.norm2.weight\", \"encoder.stages.1.1.norm2.bias\", \"encoder.stages.1.1.mlp.fc1.weight\", \"encoder.stages.1.1.mlp.fc1.bias\", \"encoder.stages.1.1.mlp.dwconv.weight\", \"encoder.stages.1.1.mlp.dwconv.bias\", \"encoder.stages.1.1.mlp.fc2.weight\", \"encoder.stages.1.1.mlp.fc2.bias\", \"encoder.stages.2.0.norm1.weight\", \"encoder.stages.2.0.norm1.bias\", \"encoder.stages.2.0.attn.relative_bias\", \"encoder.stages.2.0.attn.rel_idx\", \"encoder.stages.2.0.attn.qkv.weight\", \"encoder.stages.2.0.attn.qkv.bias\", \"encoder.stages.2.0.attn.proj.weight\", \"encoder.stages.2.0.attn.proj.bias\", \"encoder.stages.2.0.norm2.weight\", \"encoder.stages.2.0.norm2.bias\", \"encoder.stages.2.0.mlp.fc1.weight\", \"encoder.stages.2.0.mlp.fc1.bias\", \"encoder.stages.2.0.mlp.dwconv.weight\", \"encoder.stages.2.0.mlp.dwconv.bias\", \"encoder.stages.2.0.mlp.fc2.weight\", \"encoder.stages.2.0.mlp.fc2.bias\", \"encoder.stages.2.1.norm1.weight\", \"encoder.stages.2.1.norm1.bias\", \"encoder.stages.2.1.attn.relative_bias\", \"encoder.stages.2.1.attn.rel_idx\", \"encoder.stages.2.1.attn.qkv.weight\", \"encoder.stages.2.1.attn.qkv.bias\", \"encoder.stages.2.1.attn.proj.weight\", \"encoder.stages.2.1.attn.proj.bias\", \"encoder.stages.2.1.norm2.weight\", \"encoder.stages.2.1.norm2.bias\", \"encoder.stages.2.1.mlp.fc1.weight\", \"encoder.stages.2.1.mlp.fc1.bias\", \"encoder.stages.2.1.mlp.dwconv.weight\", \"encoder.stages.2.1.mlp.dwconv.bias\", \"encoder.stages.2.1.mlp.fc2.weight\", \"encoder.stages.2.1.mlp.fc2.bias\", \"encoder.stages.3.0.norm1.weight\", \"encoder.stages.3.0.norm1.bias\", \"encoder.stages.3.0.attn.relative_bias\", \"encoder.stages.3.0.attn.rel_idx\", \"encoder.stages.3.0.attn.qkv.weight\", \"encoder.stages.3.0.attn.qkv.bias\", \"encoder.stages.3.0.attn.proj.weight\", \"encoder.stages.3.0.attn.proj.bias\", \"encoder.stages.3.0.norm2.weight\", \"encoder.stages.3.0.norm2.bias\", \"encoder.stages.3.0.mlp.fc1.weight\", \"encoder.stages.3.0.mlp.fc1.bias\", \"encoder.stages.3.0.mlp.dwconv.weight\", \"encoder.stages.3.0.mlp.dwconv.bias\", \"encoder.stages.3.0.mlp.fc2.weight\", \"encoder.stages.3.0.mlp.fc2.bias\", \"encoder.stages.3.1.norm1.weight\", \"encoder.stages.3.1.norm1.bias\", \"encoder.stages.3.1.attn.relative_bias\", \"encoder.stages.3.1.attn.rel_idx\", \"encoder.stages.3.1.attn.qkv.weight\", \"encoder.stages.3.1.attn.qkv.bias\", \"encoder.stages.3.1.attn.proj.weight\", \"encoder.stages.3.1.attn.proj.bias\", \"encoder.stages.3.1.norm2.weight\", \"encoder.stages.3.1.norm2.bias\", \"encoder.stages.3.1.mlp.fc1.weight\", \"encoder.stages.3.1.mlp.fc1.bias\", \"encoder.stages.3.1.mlp.dwconv.weight\", \"encoder.stages.3.1.mlp.dwconv.bias\", \"encoder.stages.3.1.mlp.fc2.weight\", \"encoder.stages.3.1.mlp.fc2.bias\", \"encoder.norms.0.weight\", \"encoder.norms.0.bias\", \"encoder.norms.1.weight\", \"encoder.norms.1.bias\", \"encoder.norms.2.weight\", \"encoder.norms.2.bias\", \"encoder.norms.3.weight\", \"encoder.norms.3.bias\", \"decoder.decode3.attn_fuse.0.primary_conv.weight\", \"decoder.decode3.attn_fuse.0.cheap_operation.weight\", \"decoder.decode3.attn_fuse.1.weight\", \"decoder.decode3.attn_fuse.1.bias\", \"decoder.decode3.routing_mlp.2.weight\", \"decoder.decode3.routing_mlp.2.bias\", \"decoder.decode3.expert_convs.0.0.primary_conv.weight\", \"decoder.decode3.expert_convs.0.0.cheap_operation.weight\", \"decoder.decode3.expert_convs.0.1.weight\", \"decoder.decode3.expert_convs.0.1.bias\", \"decoder.decode3.expert_convs.1.0.primary_conv.weight\", \"decoder.decode3.expert_convs.1.0.cheap_operation.weight\", \"decoder.decode3.expert_convs.1.1.weight\", \"decoder.decode3.expert_convs.1.1.bias\", \"decoder.decode3.expert_convs.2.0.primary_conv.weight\", \"decoder.decode3.expert_convs.2.0.cheap_operation.weight\", \"decoder.decode3.expert_convs.2.1.weight\", \"decoder.decode3.expert_convs.2.1.bias\", \"decoder.decode3.expert_convs.3.0.primary_conv.weight\", \"decoder.decode3.expert_convs.3.0.cheap_operation.weight\", \"decoder.decode3.expert_convs.3.1.weight\", \"decoder.decode3.expert_convs.3.1.bias\", \"decoder.decode2.attn_fuse.0.primary_conv.weight\", \"decoder.decode2.attn_fuse.0.cheap_operation.weight\", \"decoder.decode2.attn_fuse.1.weight\", \"decoder.decode2.attn_fuse.1.bias\", \"decoder.decode2.routing_mlp.2.weight\", \"decoder.decode2.routing_mlp.2.bias\", \"decoder.decode2.expert_convs.0.0.primary_conv.weight\", \"decoder.decode2.expert_convs.0.0.cheap_operation.weight\", \"decoder.decode2.expert_convs.0.1.weight\", \"decoder.decode2.expert_convs.0.1.bias\", \"decoder.decode2.expert_convs.1.0.primary_conv.weight\", \"decoder.decode2.expert_convs.1.0.cheap_operation.weight\", \"decoder.decode2.expert_convs.1.1.weight\", \"decoder.decode2.expert_convs.1.1.bias\", \"decoder.decode2.expert_convs.2.0.primary_conv.weight\", \"decoder.decode2.expert_convs.2.0.cheap_operation.weight\", \"decoder.decode2.expert_convs.2.1.weight\", \"decoder.decode2.expert_convs.2.1.bias\", \"decoder.decode2.expert_convs.3.0.primary_conv.weight\", \"decoder.decode2.expert_convs.3.0.cheap_operation.weight\", \"decoder.decode2.expert_convs.3.1.weight\", \"decoder.decode2.expert_convs.3.1.bias\", \"decoder.decode1.attn_fuse.0.primary_conv.weight\", \"decoder.decode1.attn_fuse.0.cheap_operation.weight\", \"decoder.decode1.attn_fuse.1.weight\", \"decoder.decode1.attn_fuse.1.bias\", \"decoder.decode1.routing_mlp.2.weight\", \"decoder.decode1.routing_mlp.2.bias\", \"decoder.decode1.expert_convs.0.0.primary_conv.weight\", \"decoder.decode1.expert_convs.0.0.cheap_operation.weight\", \"decoder.decode1.expert_convs.0.1.weight\", \"decoder.decode1.expert_convs.0.1.bias\", \"decoder.decode1.expert_convs.1.0.primary_conv.weight\", \"decoder.decode1.expert_convs.1.0.cheap_operation.weight\", \"decoder.decode1.expert_convs.1.1.weight\", \"decoder.decode1.expert_convs.1.1.bias\", \"decoder.decode1.expert_convs.2.0.primary_conv.weight\", \"decoder.decode1.expert_convs.2.0.cheap_operation.weight\", \"decoder.decode1.expert_convs.2.1.weight\", \"decoder.decode1.expert_convs.2.1.bias\", \"decoder.decode1.expert_convs.3.0.primary_conv.weight\", \"decoder.decode1.expert_convs.3.0.cheap_operation.weight\", \"decoder.decode1.expert_convs.3.1.weight\", \"decoder.decode1.expert_convs.3.1.bias\", \"decoder.final_up.1.primary_conv.weight\", \"decoder.final_up.1.cheap_operation.weight\", \"decoder.final_up.2.weight\", \"decoder.final_up.2.bias\", \"decoder.seg_head.weight\", \"decoder.seg_head.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Model\u001b[39;00m\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m RefineFormer3D(in_channels\u001b[38;5;241m=\u001b[39mIN_CHANNELS, num_classes\u001b[38;5;241m=\u001b[39mNUM_CLASSES)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoint_epoch_124.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for RefineFormer3D:\n\tMissing key(s) in state_dict: \"patch_embed.proj.primary_conv.weight\", \"patch_embed.proj.cheap_operation.weight\", \"patch_embed.proj.se.fc.0.weight\", \"patch_embed.proj.se.fc.0.bias\", \"patch_embed.proj.se.fc.2.weight\", \"patch_embed.proj.se.fc.2.bias\", \"patch_embed.dw.weight\", \"patch_embed.dw.bias\", \"patch_embed.norm.weight\", \"patch_embed.norm.bias\", \"blocks.0.norm.weight\", \"blocks.0.norm.bias\", \"blocks.0.attn.depth.in_proj_weight\", \"blocks.0.attn.depth.in_proj_bias\", \"blocks.0.attn.depth.out_proj.weight\", \"blocks.0.attn.depth.out_proj.bias\", \"blocks.0.attn.height.in_proj_weight\", \"blocks.0.attn.height.in_proj_bias\", \"blocks.0.attn.height.out_proj.weight\", \"blocks.0.attn.height.out_proj.bias\", \"blocks.0.attn.width.in_proj_weight\", \"blocks.0.attn.width.in_proj_bias\", \"blocks.0.attn.width.out_proj.weight\", \"blocks.0.attn.width.out_proj.bias\", \"blocks.1.norm.weight\", \"blocks.1.norm.bias\", \"blocks.1.attn.depth.in_proj_weight\", \"blocks.1.attn.depth.in_proj_bias\", \"blocks.1.attn.depth.out_proj.weight\", \"blocks.1.attn.depth.out_proj.bias\", \"blocks.1.attn.height.in_proj_weight\", \"blocks.1.attn.height.in_proj_bias\", \"blocks.1.attn.height.out_proj.weight\", \"blocks.1.attn.height.out_proj.bias\", \"blocks.1.attn.width.in_proj_weight\", \"blocks.1.attn.width.in_proj_bias\", \"blocks.1.attn.width.out_proj.weight\", \"blocks.1.attn.width.out_proj.bias\", \"blocks.2.norm.weight\", \"blocks.2.norm.bias\", \"blocks.2.attn.depth.in_proj_weight\", \"blocks.2.attn.depth.in_proj_bias\", \"blocks.2.attn.depth.out_proj.weight\", \"blocks.2.attn.depth.out_proj.bias\", \"blocks.2.attn.height.in_proj_weight\", \"blocks.2.attn.height.in_proj_bias\", \"blocks.2.attn.height.out_proj.weight\", \"blocks.2.attn.height.out_proj.bias\", \"blocks.2.attn.width.in_proj_weight\", \"blocks.2.attn.width.in_proj_bias\", \"blocks.2.attn.width.out_proj.weight\", \"blocks.2.attn.width.out_proj.bias\", \"blocks.3.norm.weight\", \"blocks.3.norm.bias\", \"blocks.3.attn.depth.in_proj_weight\", \"blocks.3.attn.depth.in_proj_bias\", \"blocks.3.attn.depth.out_proj.weight\", \"blocks.3.attn.depth.out_proj.bias\", \"blocks.3.attn.height.in_proj_weight\", \"blocks.3.attn.height.in_proj_bias\", \"blocks.3.attn.height.out_proj.weight\", \"blocks.3.attn.height.out_proj.bias\", \"blocks.3.attn.width.in_proj_weight\", \"blocks.3.attn.width.in_proj_bias\", \"blocks.3.attn.width.out_proj.weight\", \"blocks.3.attn.width.out_proj.bias\", \"blocks.4.norm.weight\", \"blocks.4.norm.bias\", \"blocks.4.attn.depth.in_proj_weight\", \"blocks.4.attn.depth.in_proj_bias\", \"blocks.4.attn.depth.out_proj.weight\", \"blocks.4.attn.depth.out_proj.bias\", \"blocks.4.attn.height.in_proj_weight\", \"blocks.4.attn.height.in_proj_bias\", \"blocks.4.attn.height.out_proj.weight\", \"blocks.4.attn.height.out_proj.bias\", \"blocks.4.attn.width.in_proj_weight\", \"blocks.4.attn.width.in_proj_bias\", \"blocks.4.attn.width.out_proj.weight\", \"blocks.4.attn.width.out_proj.bias\", \"blocks.5.norm.weight\", \"blocks.5.norm.bias\", \"blocks.5.attn.depth.in_proj_weight\", \"blocks.5.attn.depth.in_proj_bias\", \"blocks.5.attn.depth.out_proj.weight\", \"blocks.5.attn.depth.out_proj.bias\", \"blocks.5.attn.height.in_proj_weight\", \"blocks.5.attn.height.in_proj_bias\", \"blocks.5.attn.height.out_proj.weight\", \"blocks.5.attn.height.out_proj.bias\", \"blocks.5.attn.width.in_proj_weight\", \"blocks.5.attn.width.in_proj_bias\", \"blocks.5.attn.width.out_proj.weight\", \"blocks.5.attn.width.out_proj.bias\", \"decoder.conv.primary_conv.weight\", \"decoder.conv.cheap_operation.weight\", \"decoder.conv.se.fc.0.weight\", \"decoder.conv.se.fc.0.bias\", \"decoder.conv.se.fc.2.weight\", \"decoder.conv.se.fc.2.bias\", \"decoder.norm.weight\", \"decoder.norm.bias\", \"decoder.head.weight\", \"decoder.head.bias\", \"aux_head.weight\", \"aux_head.bias\", \"boundary_head.conv.weight\", \"boundary_head.conv.bias\". \n\tUnexpected key(s) in state_dict: \"encoder.embeds.0.proj.primary_conv.weight\", \"encoder.embeds.0.proj.cheap_operation.weight\", \"encoder.embeds.0.norm.weight\", \"encoder.embeds.0.norm.bias\", \"encoder.embeds.1.proj.primary_conv.weight\", \"encoder.embeds.1.proj.cheap_operation.weight\", \"encoder.embeds.1.norm.weight\", \"encoder.embeds.1.norm.bias\", \"encoder.embeds.2.proj.primary_conv.weight\", \"encoder.embeds.2.proj.cheap_operation.weight\", \"encoder.embeds.2.norm.weight\", \"encoder.embeds.2.norm.bias\", \"encoder.embeds.3.proj.primary_conv.weight\", \"encoder.embeds.3.proj.cheap_operation.weight\", \"encoder.embeds.3.norm.weight\", \"encoder.embeds.3.norm.bias\", \"encoder.stages.0.0.norm1.weight\", \"encoder.stages.0.0.norm1.bias\", \"encoder.stages.0.0.attn.relative_bias\", \"encoder.stages.0.0.attn.rel_idx\", \"encoder.stages.0.0.attn.qkv.weight\", \"encoder.stages.0.0.attn.qkv.bias\", \"encoder.stages.0.0.attn.proj.weight\", \"encoder.stages.0.0.attn.proj.bias\", \"encoder.stages.0.0.norm2.weight\", \"encoder.stages.0.0.norm2.bias\", \"encoder.stages.0.0.mlp.fc1.weight\", \"encoder.stages.0.0.mlp.fc1.bias\", \"encoder.stages.0.0.mlp.dwconv.weight\", \"encoder.stages.0.0.mlp.dwconv.bias\", \"encoder.stages.0.0.mlp.fc2.weight\", \"encoder.stages.0.0.mlp.fc2.bias\", \"encoder.stages.0.1.norm1.weight\", \"encoder.stages.0.1.norm1.bias\", \"encoder.stages.0.1.attn.relative_bias\", \"encoder.stages.0.1.attn.rel_idx\", \"encoder.stages.0.1.attn.qkv.weight\", \"encoder.stages.0.1.attn.qkv.bias\", \"encoder.stages.0.1.attn.proj.weight\", \"encoder.stages.0.1.attn.proj.bias\", \"encoder.stages.0.1.norm2.weight\", \"encoder.stages.0.1.norm2.bias\", \"encoder.stages.0.1.mlp.fc1.weight\", \"encoder.stages.0.1.mlp.fc1.bias\", \"encoder.stages.0.1.mlp.dwconv.weight\", \"encoder.stages.0.1.mlp.dwconv.bias\", \"encoder.stages.0.1.mlp.fc2.weight\", \"encoder.stages.0.1.mlp.fc2.bias\", \"encoder.stages.1.0.norm1.weight\", \"encoder.stages.1.0.norm1.bias\", \"encoder.stages.1.0.attn.relative_bias\", \"encoder.stages.1.0.attn.rel_idx\", \"encoder.stages.1.0.attn.qkv.weight\", \"encoder.stages.1.0.attn.qkv.bias\", \"encoder.stages.1.0.attn.proj.weight\", \"encoder.stages.1.0.attn.proj.bias\", \"encoder.stages.1.0.norm2.weight\", \"encoder.stages.1.0.norm2.bias\", \"encoder.stages.1.0.mlp.fc1.weight\", \"encoder.stages.1.0.mlp.fc1.bias\", \"encoder.stages.1.0.mlp.dwconv.weight\", \"encoder.stages.1.0.mlp.dwconv.bias\", \"encoder.stages.1.0.mlp.fc2.weight\", \"encoder.stages.1.0.mlp.fc2.bias\", \"encoder.stages.1.1.norm1.weight\", \"encoder.stages.1.1.norm1.bias\", \"encoder.stages.1.1.attn.relative_bias\", \"encoder.stages.1.1.attn.rel_idx\", \"encoder.stages.1.1.attn.qkv.weight\", \"encoder.stages.1.1.attn.qkv.bias\", \"encoder.stages.1.1.attn.proj.weight\", \"encoder.stages.1.1.attn.proj.bias\", \"encoder.stages.1.1.norm2.weight\", \"encoder.stages.1.1.norm2.bias\", \"encoder.stages.1.1.mlp.fc1.weight\", \"encoder.stages.1.1.mlp.fc1.bias\", \"encoder.stages.1.1.mlp.dwconv.weight\", \"encoder.stages.1.1.mlp.dwconv.bias\", \"encoder.stages.1.1.mlp.fc2.weight\", \"encoder.stages.1.1.mlp.fc2.bias\", \"encoder.stages.2.0.norm1.weight\", \"encoder.stages.2.0.norm1.bias\", \"encoder.stages.2.0.attn.relative_bias\", \"encoder.stages.2.0.attn.rel_idx\", \"encoder.stages.2.0.attn.qkv.weight\", \"encoder.stages.2.0.attn.qkv.bias\", \"encoder.stages.2.0.attn.proj.weight\", \"encoder.stages.2.0.attn.proj.bias\", \"encoder.stages.2.0.norm2.weight\", \"encoder.stages.2.0.norm2.bias\", \"encoder.stages.2.0.mlp.fc1.weight\", \"encoder.stages.2.0.mlp.fc1.bias\", \"encoder.stages.2.0.mlp.dwconv.weight\", \"encoder.stages.2.0.mlp.dwconv.bias\", \"encoder.stages.2.0.mlp.fc2.weight\", \"encoder.stages.2.0.mlp.fc2.bias\", \"encoder.stages.2.1.norm1.weight\", \"encoder.stages.2.1.norm1.bias\", \"encoder.stages.2.1.attn.relative_bias\", \"encoder.stages.2.1.attn.rel_idx\", \"encoder.stages.2.1.attn.qkv.weight\", \"encoder.stages.2.1.attn.qkv.bias\", \"encoder.stages.2.1.attn.proj.weight\", \"encoder.stages.2.1.attn.proj.bias\", \"encoder.stages.2.1.norm2.weight\", \"encoder.stages.2.1.norm2.bias\", \"encoder.stages.2.1.mlp.fc1.weight\", \"encoder.stages.2.1.mlp.fc1.bias\", \"encoder.stages.2.1.mlp.dwconv.weight\", \"encoder.stages.2.1.mlp.dwconv.bias\", \"encoder.stages.2.1.mlp.fc2.weight\", \"encoder.stages.2.1.mlp.fc2.bias\", \"encoder.stages.3.0.norm1.weight\", \"encoder.stages.3.0.norm1.bias\", \"encoder.stages.3.0.attn.relative_bias\", \"encoder.stages.3.0.attn.rel_idx\", \"encoder.stages.3.0.attn.qkv.weight\", \"encoder.stages.3.0.attn.qkv.bias\", \"encoder.stages.3.0.attn.proj.weight\", \"encoder.stages.3.0.attn.proj.bias\", \"encoder.stages.3.0.norm2.weight\", \"encoder.stages.3.0.norm2.bias\", \"encoder.stages.3.0.mlp.fc1.weight\", \"encoder.stages.3.0.mlp.fc1.bias\", \"encoder.stages.3.0.mlp.dwconv.weight\", \"encoder.stages.3.0.mlp.dwconv.bias\", \"encoder.stages.3.0.mlp.fc2.weight\", \"encoder.stages.3.0.mlp.fc2.bias\", \"encoder.stages.3.1.norm1.weight\", \"encoder.stages.3.1.norm1.bias\", \"encoder.stages.3.1.attn.relative_bias\", \"encoder.stages.3.1.attn.rel_idx\", \"encoder.stages.3.1.attn.qkv.weight\", \"encoder.stages.3.1.attn.qkv.bias\", \"encoder.stages.3.1.attn.proj.weight\", \"encoder.stages.3.1.attn.proj.bias\", \"encoder.stages.3.1.norm2.weight\", \"encoder.stages.3.1.norm2.bias\", \"encoder.stages.3.1.mlp.fc1.weight\", \"encoder.stages.3.1.mlp.fc1.bias\", \"encoder.stages.3.1.mlp.dwconv.weight\", \"encoder.stages.3.1.mlp.dwconv.bias\", \"encoder.stages.3.1.mlp.fc2.weight\", \"encoder.stages.3.1.mlp.fc2.bias\", \"encoder.norms.0.weight\", \"encoder.norms.0.bias\", \"encoder.norms.1.weight\", \"encoder.norms.1.bias\", \"encoder.norms.2.weight\", \"encoder.norms.2.bias\", \"encoder.norms.3.weight\", \"encoder.norms.3.bias\", \"decoder.decode3.attn_fuse.0.primary_conv.weight\", \"decoder.decode3.attn_fuse.0.cheap_operation.weight\", \"decoder.decode3.attn_fuse.1.weight\", \"decoder.decode3.attn_fuse.1.bias\", \"decoder.decode3.routing_mlp.2.weight\", \"decoder.decode3.routing_mlp.2.bias\", \"decoder.decode3.expert_convs.0.0.primary_conv.weight\", \"decoder.decode3.expert_convs.0.0.cheap_operation.weight\", \"decoder.decode3.expert_convs.0.1.weight\", \"decoder.decode3.expert_convs.0.1.bias\", \"decoder.decode3.expert_convs.1.0.primary_conv.weight\", \"decoder.decode3.expert_convs.1.0.cheap_operation.weight\", \"decoder.decode3.expert_convs.1.1.weight\", \"decoder.decode3.expert_convs.1.1.bias\", \"decoder.decode3.expert_convs.2.0.primary_conv.weight\", \"decoder.decode3.expert_convs.2.0.cheap_operation.weight\", \"decoder.decode3.expert_convs.2.1.weight\", \"decoder.decode3.expert_convs.2.1.bias\", \"decoder.decode3.expert_convs.3.0.primary_conv.weight\", \"decoder.decode3.expert_convs.3.0.cheap_operation.weight\", \"decoder.decode3.expert_convs.3.1.weight\", \"decoder.decode3.expert_convs.3.1.bias\", \"decoder.decode2.attn_fuse.0.primary_conv.weight\", \"decoder.decode2.attn_fuse.0.cheap_operation.weight\", \"decoder.decode2.attn_fuse.1.weight\", \"decoder.decode2.attn_fuse.1.bias\", \"decoder.decode2.routing_mlp.2.weight\", \"decoder.decode2.routing_mlp.2.bias\", \"decoder.decode2.expert_convs.0.0.primary_conv.weight\", \"decoder.decode2.expert_convs.0.0.cheap_operation.weight\", \"decoder.decode2.expert_convs.0.1.weight\", \"decoder.decode2.expert_convs.0.1.bias\", \"decoder.decode2.expert_convs.1.0.primary_conv.weight\", \"decoder.decode2.expert_convs.1.0.cheap_operation.weight\", \"decoder.decode2.expert_convs.1.1.weight\", \"decoder.decode2.expert_convs.1.1.bias\", \"decoder.decode2.expert_convs.2.0.primary_conv.weight\", \"decoder.decode2.expert_convs.2.0.cheap_operation.weight\", \"decoder.decode2.expert_convs.2.1.weight\", \"decoder.decode2.expert_convs.2.1.bias\", \"decoder.decode2.expert_convs.3.0.primary_conv.weight\", \"decoder.decode2.expert_convs.3.0.cheap_operation.weight\", \"decoder.decode2.expert_convs.3.1.weight\", \"decoder.decode2.expert_convs.3.1.bias\", \"decoder.decode1.attn_fuse.0.primary_conv.weight\", \"decoder.decode1.attn_fuse.0.cheap_operation.weight\", \"decoder.decode1.attn_fuse.1.weight\", \"decoder.decode1.attn_fuse.1.bias\", \"decoder.decode1.routing_mlp.2.weight\", \"decoder.decode1.routing_mlp.2.bias\", \"decoder.decode1.expert_convs.0.0.primary_conv.weight\", \"decoder.decode1.expert_convs.0.0.cheap_operation.weight\", \"decoder.decode1.expert_convs.0.1.weight\", \"decoder.decode1.expert_convs.0.1.bias\", \"decoder.decode1.expert_convs.1.0.primary_conv.weight\", \"decoder.decode1.expert_convs.1.0.cheap_operation.weight\", \"decoder.decode1.expert_convs.1.1.weight\", \"decoder.decode1.expert_convs.1.1.bias\", \"decoder.decode1.expert_convs.2.0.primary_conv.weight\", \"decoder.decode1.expert_convs.2.0.cheap_operation.weight\", \"decoder.decode1.expert_convs.2.1.weight\", \"decoder.decode1.expert_convs.2.1.bias\", \"decoder.decode1.expert_convs.3.0.primary_conv.weight\", \"decoder.decode1.expert_convs.3.0.cheap_operation.weight\", \"decoder.decode1.expert_convs.3.1.weight\", \"decoder.decode1.expert_convs.3.1.bias\", \"decoder.final_up.1.primary_conv.weight\", \"decoder.final_up.1.cheap_operation.weight\", \"decoder.final_up.2.weight\", \"decoder.final_up.2.bias\", \"decoder.seg_head.weight\", \"decoder.seg_head.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from model3 import RefineFormer3D\n",
    "from dataset import BraTSDataset\n",
    "from config import IN_CHANNELS, NUM_CLASSES\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = torch.device('cpu')\n",
    "\n",
    "# Dataset & loader\n",
    "test_dataset = BraTSDataset(\n",
    "    root_dirs=[\"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BRATS_SPLIT/val\"],\n",
    "    transform=None,\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=14)\n",
    "\n",
    "# Model\n",
    "model = RefineFormer3D(in_channels=IN_CHANNELS, num_classes=NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(\"checkpoint_epoch_124.pt\", map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Metrics\n",
    "dice_list, iou_list = [], []\n",
    "wt_dice_list, tc_dice_list, et_dice_list = [], [], []\n",
    "total_correct, total_voxels = 0, 0\n",
    "\n",
    "def dice_score(pred, target):\n",
    "    intersection = (pred & target).sum()\n",
    "    union = pred.sum() + target.sum()\n",
    "    return (2 * intersection + 1e-5) / (union + 1e-5)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        targets = targets.to(DEVICE, dtype=torch.long)\n",
    "\n",
    "        outputs = model(inputs)['main']\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Overall Accuracy\n",
    "        total_correct += (preds == targets).sum().item()\n",
    "        total_voxels += torch.numel(preds)\n",
    "\n",
    "        for i in range(preds.shape[0]):\n",
    "            pred = preds[i].cpu().numpy()\n",
    "            gt = targets[i].cpu().numpy()\n",
    "\n",
    "            # One-hot masks for dice per class\n",
    "            for c in range(NUM_CLASSES):\n",
    "                dice = dice_score((pred == c), (gt == c))\n",
    "                dice_list.append(dice)\n",
    "\n",
    "            # WT = labels 1, 2, 3\n",
    "            wt_dice = dice_score((pred > 0), (gt > 0))\n",
    "            wt_dice_list.append(wt_dice)\n",
    "\n",
    "            # TC = labels 1 and 3\n",
    "            tc_dice = dice_score(((pred == 1) | (pred == 3)), ((gt == 1) | (gt == 3)))\n",
    "            tc_dice_list.append(tc_dice)\n",
    "\n",
    "            # ET = label 3\n",
    "            et_dice = dice_score((pred == 3), (gt == 3))\n",
    "            et_dice_list.append(et_dice)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\n📊 Evaluation Results:\")\n",
    "print(f\"Avg Dice Score (mean of all classes): {np.mean(dice_list):.4f}\")\n",
    "print(f\"WT Dice (Whole Tumor): {np.mean(wt_dice_list)*100:.2f}%\")\n",
    "print(f\"TC Dice (Tumor Core): {np.mean(tc_dice_list)*100:.2f}%\")\n",
    "print(f\"ET Dice (Enhancing Tumor): {np.mean(et_dice_list)*100:.2f}%\")\n",
    "print(f\"Overall Accuracy: {(total_correct / total_voxels)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cb9e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4998c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total valid patient directories found: 73\n",
      "Loading 0/108 layers from checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152291/423769458.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"checkpoint_epoch_124.pt\", map_location=DEVICE)\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.2089\n",
      "WT Dice (Whole Tumor):           8.38%\n",
      "TC Dice (Tumor Core):            5.30%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Overall Accuracy:                37.71%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from model3 import RefineFormer3D\n",
    "from dataset import BraTSDataset\n",
    "from config import IN_CHANNELS, NUM_CLASSES\n",
    "import numpy as np\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device('cpu')\n",
    "\n",
    "# Dataset & loader\n",
    "test_dataset = BraTSDataset(\n",
    "    root_dirs=[\"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BRATS_SPLIT/val\"],\n",
    "    transform=None,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=14,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = RefineFormer3D(in_channels=IN_CHANNELS, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Robustly load checkpoint: only load matching keys\n",
    "ckpt = torch.load(\"checkpoint_epoch_124.pt\", map_location=DEVICE)\n",
    "model_dict = model.state_dict()\n",
    "# filter out unmatched keys\n",
    "pretrained_dict = {k: v for k, v in ckpt.items() if k in model_dict}\n",
    "print(f\"Loading {len(pretrained_dict)}/{len(model_dict)} layers from checkpoint\")\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Dice / accuracy helpers\n",
    "def dice_score(pred, target):\n",
    "    intersection = (pred & target).sum()\n",
    "    union = pred.sum() + target.sum()\n",
    "    return (2 * intersection + 1e-5) / (union + 1e-5)\n",
    "\n",
    "# Accumulators\n",
    "dice_list = []\n",
    "wt_dice_list = []\n",
    "tc_dice_list = []\n",
    "et_dice_list = []\n",
    "total_correct = 0\n",
    "total_voxels = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        targets = targets.to(DEVICE, dtype=torch.long)\n",
    "\n",
    "        outputs = model(inputs)[\"main\"]\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Overall Accuracy\n",
    "        total_correct += (preds == targets).sum().item()\n",
    "        total_voxels += preds.numel()\n",
    "\n",
    "        # Per‐sample Dice\n",
    "        for i in range(preds.shape[0]):\n",
    "            p = preds[i].cpu().numpy()\n",
    "            g = targets[i].cpu().numpy()\n",
    "            # per‐class dice\n",
    "            for c in range(NUM_CLASSES):\n",
    "                dice_list.append(dice_score(p == c, g == c))\n",
    "            # WT (classes>0)\n",
    "            wt_dice_list.append(dice_score(p > 0, g > 0))\n",
    "            # TC (classes 1 or 3)\n",
    "            tc_dice_list.append(dice_score((p == 1) | (p == 3), (g == 1) | (g == 3)))\n",
    "            # ET (class 3)\n",
    "            et_dice_list.append(dice_score(p == 3, g == 3))\n",
    "\n",
    "# Print final results\n",
    "print(\"\\n📊 Evaluation Results:\")\n",
    "print(f\"Avg Dice (mean over all classes): {np.mean(dice_list):.4f}\")\n",
    "print(f\"WT Dice (Whole Tumor):           {np.mean(wt_dice_list)*100:.2f}%\")\n",
    "print(f\"TC Dice (Tumor Core):            {np.mean(tc_dice_list)*100:.2f}%\")\n",
    "print(f\"ET Dice (Enhancing Tumor):       {np.mean(et_dice_list)*100:.2f}%\")\n",
    "print(f\"Overall Accuracy:                {(total_correct/total_voxels)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a0f2ae",
   "metadata": {},
   "source": [
    "MAIN SCRIPT ------- TRAIN + VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2898c6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total valid patient directories found: 411\n",
      "✅ Total valid patient directories found: 73\n",
      "\n",
      "--- Epoch [1/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/411 [00:00<?, ?it/s]/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "                                                 \r"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 5.78 GiB of which 1.75 GiB is free. Including non-PyTorch memory, this process has 3.27 GiB memory in use. Of the allocated memory 3.10 GiB is allocated by PyTorch, and 31.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 147\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Final model saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 136\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, NUM_EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Epoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 136\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     val_loss, val_dice \u001b[38;5;241m=\u001b[39m validate(model, val_loader, criterion, device)\n\u001b[1;32m    138\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[1], line 87\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, device, scaler)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     84\u001b[0m \n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# --- Test-Time Augmentation (TTA) ---\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     flipped_inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflip(inputs, dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# flip along depth\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     logits_orig \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     88\u001b[0m     logits_flip \u001b[38;5;241m=\u001b[39m model(flipped_inputs)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     89\u001b[0m     logits_flip \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflip(logits_flip, dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# flip back\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/model4_ghost_upgraded/model5_ghost_upg.py:458\u001b[0m, in \u001b[0;36mRefineFormer3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    457\u001b[0m     feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/model4_ghost_upgraded/model5_ghost_upg.py:415\u001b[0m, in \u001b[0;36mRefinedDecoderHead3D.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    413\u001b[0m c1, c2, c3, c4 \u001b[38;5;241m=\u001b[39m features\n\u001b[1;32m    414\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode3(c4, c3)\n\u001b[0;32m--> 415\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode1(x, c1)\n\u001b[1;32m    417\u001b[0m aux_out2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maux2(x)  \u001b[38;5;66;03m# From decode2 stage\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/model4_ghost_upgraded/model5_ghost_upg.py:389\u001b[0m, in \u001b[0;36mDecoderBlock3D.forward\u001b[0;34m(self, x, skip)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, skip):\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;66;03m# upsample + skip connect\u001b[39;00m\n\u001b[1;32m    388\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(x, size\u001b[38;5;241m=\u001b[39mskip\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:], mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 389\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_fuse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/model4_ghost_upgraded/model5_ghost_upg.py:39\u001b[0m, in \u001b[0;36mCrossFuse.forward\u001b[0;34m(self, x, skip)\u001b[0m\n\u001b[1;32m     36\u001b[0m k, v \u001b[38;5;241m=\u001b[39m kv\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)                   \u001b[38;5;66;03m# each [B, N, in_ch]\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Scaled dot-product attention\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_ch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, N, N]\u001b[39;00m\n\u001b[1;32m     40\u001b[0m attn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(attn, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Fuse\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 5.78 GiB of which 1.75 GiB is free. Including non-PyTorch memory, this process has 3.27 GiB memory in use. Of the allocated memory 3.10 GiB is allocated by PyTorch, and 31.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "from model5_ghost_upg import RefineFormer3D\n",
    "from dataset import BraTSDataset\n",
    "from optimizer import get_optimizer, get_scheduler\n",
    "from augmentation import Compose3D, RandomFlip3D, RandomRotation3D, RandomNoise3D, TumorCoreContrast\n",
    "from losses import RefineFormer3DLoss\n",
    "from config import DEVICE, IN_CHANNELS, NUM_CLASSES, BASE_LR, WEIGHT_DECAY, NUM_EPOCHS, ATTN_DROP_RATE\n",
    "\n",
    "def pad_input_for_windows(x, window_size=(2, 2, 2)):\n",
    "    _, _, D, H, W = x.shape\n",
    "    pad_d = (window_size[0] - D % window_size[0]) % window_size[0]\n",
    "    pad_h = (window_size[1] - H % window_size[1]) % window_size[1]\n",
    "    pad_w = (window_size[2] - W % window_size[2]) % window_size[2]\n",
    "    return F.pad(x, (0, pad_w, 0, pad_h, 0, pad_d))\n",
    "\n",
    "def dice_score(pred, target):\n",
    "    intersection = (pred & target).sum()\n",
    "    union = pred.sum() + target.sum()\n",
    "    return (2 * intersection + 1e-5) / (union + 1e-5)\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    dice_list, wt_dice_list, tc_dice_list, et_dice_list = [], [], [], []\n",
    "    total_correct, total_voxels = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            inputs = pad_input_for_windows(inputs, window_size=(2, 2, 2))\n",
    "            targets = targets.to(device=device, dtype=torch.long)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            preds = torch.argmax(outputs[\"main\"], dim=1)\n",
    "\n",
    "            total_correct += (preds == targets).sum().item()\n",
    "            total_voxels += preds.numel()\n",
    "\n",
    "            for i in range(preds.shape[0]):\n",
    "                p = preds[i].cpu().numpy()\n",
    "                g = targets[i].cpu().numpy()\n",
    "                for c in range(NUM_CLASSES):\n",
    "                    dice_list.append(dice_score(p == c, g == c))\n",
    "                wt_dice_list.append(dice_score(p > 0, g > 0))\n",
    "                tc_dice_list.append(dice_score((p == 1) | (p == 3), (g == 1) | (g == 3)))\n",
    "                et_dice_list.append(dice_score(p == 3, g == 3))\n",
    "\n",
    "    avg_loss = val_loss / len(dataloader.dataset)\n",
    "    avg_dice = np.mean(dice_list)\n",
    "    acc = total_correct / total_voxels\n",
    "    print(\"--- Evaluation Results:\")\n",
    "    print(f\"Avg Dice (mean over all classes): {avg_dice:.4f}\")\n",
    "    print(f\"WT Dice (Whole Tumor):           {np.mean(wt_dice_list)*100:.2f}%\")\n",
    "    print(f\"TC Dice (Tumor Core):            {np.mean(tc_dice_list)*100:.2f}%\")\n",
    "    print(f\"ET Dice (Enhancing Tumor):       {np.mean(et_dice_list)*100:.2f}%\")\n",
    "    print(f\"Per-Class Dice: {[f'{d*100:.2f}%' for d in dice_list[:NUM_CLASSES]]}\")\n",
    "    print(f\"Overall Accuracy:                {acc*100:.2f}%\")\n",
    "    return avg_loss, avg_dice\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, scaler):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, targets in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        inputs = pad_input_for_windows(inputs, window_size=(2, 2, 2))\n",
    "        targets = targets.to(device=device, dtype=torch.long, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device_type='cuda'):\n",
    "\n",
    "        # --- Test-Time Augmentation (TTA) ---\n",
    "            flipped_inputs = torch.flip(inputs, dims=[2])  # flip along depth\n",
    "            logits_orig = model(inputs)[\"main\"]\n",
    "            logits_flip = model(flipped_inputs)[\"main\"]\n",
    "            logits_flip = torch.flip(logits_flip, dims=[2])  # flip back\n",
    "\n",
    "            # Average predictions\n",
    "            logits = (logits_orig + logits_flip) / 2.0\n",
    "            outputs = {\"main\": logits}\n",
    "\n",
    "\n",
    "            if torch.isnan(outputs[\"main\"]).any():\n",
    "                print(\" Model output contains NaNs \")\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "def main():\n",
    "    device = DEVICE\n",
    "    model = RefineFormer3D(in_channels=IN_CHANNELS, num_classes=NUM_CLASSES, attn_drop_rate=ATTN_DROP_RATE).to(device)\n",
    "    optimizer = get_optimizer(model, base_lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "    criterion = RefineFormer3DLoss()\n",
    "    scaler = GradScaler(device='cuda')\n",
    "\n",
    "    train_transform = Compose3D([\n",
    "        RandomFlip3D(p=0.5),\n",
    "        RandomRotation3D(p=0.5),\n",
    "        RandomNoise3D(p=0.3),\n",
    "        TumorCoreContrast(p=0.5, scale=1.5),\n",
    "    ])\n",
    "\n",
    "    train_dataset = BraTSDataset(\n",
    "        root_dirs=[\"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BRATS_SPLIT/train\"],\n",
    "        transform=train_transform,\n",
    "    )\n",
    "    val_dataset = BraTSDataset(\n",
    "        root_dirs=[\"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BRATS_SPLIT/val\"],\n",
    "        transform=None,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=14, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=12, pin_memory=True)\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        print(f\"\\n--- Epoch [{epoch}/{NUM_EPOCHS}] ---\")\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, scaler)\n",
    "        val_loss, val_dice = validate(model, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f} | Avg Dice: {val_dice:.4f}\")\n",
    "        torch.save(model.state_dict(), f\"checkpoint_epoch_{epoch}.pt\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"final_model.pt\")\n",
    "    print(\" Final model saved as 'final_model.pt'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7e3d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total valid patient directories found: 411\n",
      "✅ Total valid patient directories found: 73\n",
      "\n",
      "--- Epoch [1/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/411 [00:00<?, ?it/s]/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]         /mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.6544\n",
      "WT Dice (Whole Tumor):           73.49%\n",
      "TC Dice (Tumor Core):            55.58%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.40%', '49.97%', '0.00%']\n",
      "Overall Accuracy:                96.66%\n",
      "Train Loss: 2.6926 | Validation Loss: 2.3231 | Avg Dice: 0.6544\n",
      "✅ New best model saved (best Dice: 0.6544)\n",
      "\n",
      "--- Epoch [2/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.7360\n",
      "WT Dice (Whole Tumor):           81.02%\n",
      "TC Dice (Tumor Core):            65.38%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.77%', '64.36%', '32.34%']\n",
      "Overall Accuracy:                98.03%\n",
      "Train Loss: 2.1635 | Validation Loss: 2.0300 | Avg Dice: 0.7360\n",
      "✅ New best model saved (best Dice: 0.7360)\n",
      "\n",
      "--- Epoch [3/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.7697\n",
      "WT Dice (Whole Tumor):           83.99%\n",
      "TC Dice (Tumor Core):            66.70%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.82%', '78.22%', '63.94%']\n",
      "Overall Accuracy:                98.16%\n",
      "Train Loss: 1.9029 | Validation Loss: 1.7812 | Avg Dice: 0.7697\n",
      "✅ New best model saved (best Dice: 0.7697)\n",
      "\n",
      "--- Epoch [4/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.7739\n",
      "WT Dice (Whole Tumor):           83.51%\n",
      "TC Dice (Tumor Core):            69.75%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.68%', '69.84%', '59.25%']\n",
      "Overall Accuracy:                98.20%\n",
      "Train Loss: 1.7057 | Validation Loss: 1.5782 | Avg Dice: 0.7739\n",
      "✅ New best model saved (best Dice: 0.7739)\n",
      "\n",
      "--- Epoch [5/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.7902\n",
      "WT Dice (Whole Tumor):           84.49%\n",
      "TC Dice (Tumor Core):            70.27%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.78%', '74.79%', '65.39%']\n",
      "Overall Accuracy:                98.33%\n",
      "Train Loss: 1.5447 | Validation Loss: 1.4514 | Avg Dice: 0.7902\n",
      "✅ New best model saved (best Dice: 0.7902)\n",
      "\n",
      "--- Epoch [6/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.7772\n",
      "WT Dice (Whole Tumor):           81.49%\n",
      "TC Dice (Tumor Core):            68.39%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.58%', '62.77%', '59.51%']\n",
      "Overall Accuracy:                98.11%\n",
      "Train Loss: 1.4015 | Validation Loss: 1.3072 | Avg Dice: 0.7772\n",
      "\n",
      "--- Epoch [7/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.8097\n",
      "WT Dice (Whole Tumor):           86.40%\n",
      "TC Dice (Tumor Core):            72.39%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.79%', '76.77%', '73.15%']\n",
      "Overall Accuracy:                98.53%\n",
      "Train Loss: 1.2764 | Validation Loss: 1.1592 | Avg Dice: 0.8097\n",
      "✅ New best model saved (best Dice: 0.8097)\n",
      "\n",
      "--- Epoch [8/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.7995\n",
      "WT Dice (Whole Tumor):           84.04%\n",
      "TC Dice (Tumor Core):            70.42%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.44%', '53.89%', '68.59%']\n",
      "Overall Accuracy:                98.38%\n",
      "Train Loss: 1.1509 | Validation Loss: 1.0617 | Avg Dice: 0.7995\n",
      "\n",
      "--- Epoch [9/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.8153\n",
      "WT Dice (Whole Tumor):           86.84%\n",
      "TC Dice (Tumor Core):            73.55%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.86%', '79.76%', '75.97%']\n",
      "Overall Accuracy:                98.57%\n",
      "Train Loss: 1.0729 | Validation Loss: 0.9759 | Avg Dice: 0.8153\n",
      "✅ New best model saved (best Dice: 0.8153)\n",
      "\n",
      "--- Epoch [10/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.8144\n",
      "WT Dice (Whole Tumor):           86.32%\n",
      "TC Dice (Tumor Core):            72.39%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.84%', '79.98%', '76.63%']\n",
      "Overall Accuracy:                98.52%\n",
      "Train Loss: 0.9734 | Validation Loss: 0.9032 | Avg Dice: 0.8144\n",
      "\n",
      "--- Epoch [11/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.8308\n",
      "WT Dice (Whole Tumor):           87.44%\n",
      "TC Dice (Tumor Core):            74.79%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.78%', '76.00%', '74.54%']\n",
      "Overall Accuracy:                98.68%\n",
      "Train Loss: 0.9024 | Validation Loss: 0.8012 | Avg Dice: 0.8308\n",
      "✅ New best model saved (best Dice: 0.8308)\n",
      "\n",
      "--- Epoch [12/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.8261\n",
      "WT Dice (Whole Tumor):           87.27%\n",
      "TC Dice (Tumor Core):            74.68%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.66%', '67.29%', '70.72%']\n",
      "Overall Accuracy:                98.71%\n",
      "Train Loss: 0.8252 | Validation Loss: 0.7567 | Avg Dice: 0.8261\n",
      "\n",
      "--- Epoch [13/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.8289\n",
      "WT Dice (Whole Tumor):           88.15%\n",
      "TC Dice (Tumor Core):            74.50%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.85%', '80.56%', '77.02%']\n",
      "Overall Accuracy:                98.72%\n",
      "Train Loss: 0.7814 | Validation Loss: 0.7144 | Avg Dice: 0.8289\n",
      "\n",
      "--- Epoch [14/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.8321\n",
      "WT Dice (Whole Tumor):           88.28%\n",
      "TC Dice (Tumor Core):            76.24%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.81%', '75.47%', '73.93%']\n",
      "Overall Accuracy:                98.77%\n",
      "Train Loss: 0.7553 | Validation Loss: 0.6759 | Avg Dice: 0.8321\n",
      "✅ New best model saved (best Dice: 0.8321)\n",
      "\n",
      "--- Epoch [15/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.8363\n",
      "WT Dice (Whole Tumor):           88.21%\n",
      "TC Dice (Tumor Core):            75.75%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.79%', '74.65%', '77.93%']\n",
      "Overall Accuracy:                98.78%\n",
      "Train Loss: 0.6908 | Validation Loss: 0.6486 | Avg Dice: 0.8363\n",
      "✅ New best model saved (best Dice: 0.8363)\n",
      "\n",
      "--- Epoch [16/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.8392\n",
      "WT Dice (Whole Tumor):           88.35%\n",
      "TC Dice (Tumor Core):            75.89%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.87%', '81.37%', '81.07%']\n",
      "Overall Accuracy:                98.76%\n",
      "Train Loss: 0.6803 | Validation Loss: 0.6327 | Avg Dice: 0.8392\n",
      "✅ New best model saved (best Dice: 0.8392)\n",
      "\n",
      "--- Epoch [17/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.8297\n",
      "WT Dice (Whole Tumor):           87.05%\n",
      "TC Dice (Tumor Core):            74.28%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.89%', '81.83%', '79.76%']\n",
      "Overall Accuracy:                98.68%\n",
      "Train Loss: 0.6592 | Validation Loss: 0.6417 | Avg Dice: 0.8297\n",
      "\n",
      "--- Epoch [18/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.8424\n",
      "WT Dice (Whole Tumor):           88.29%\n",
      "TC Dice (Tumor Core):            76.06%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.87%', '82.09%', '82.26%']\n",
      "Overall Accuracy:                98.78%\n",
      "Train Loss: 0.6219 | Validation Loss: 0.6039 | Avg Dice: 0.8424\n",
      "✅ New best model saved (best Dice: 0.8424)\n",
      "\n",
      "--- Epoch [19/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.8365\n",
      "WT Dice (Whole Tumor):           88.59%\n",
      "TC Dice (Tumor Core):            74.83%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.89%', '83.13%', '82.28%']\n",
      "Overall Accuracy:                98.78%\n",
      "Train Loss: 0.6147 | Validation Loss: 0.6052 | Avg Dice: 0.8365\n",
      "\n",
      "--- Epoch [20/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.8298\n",
      "WT Dice (Whole Tumor):           88.98%\n",
      "TC Dice (Tumor Core):            74.15%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.84%', '79.21%', '73.50%']\n",
      "Overall Accuracy:                98.75%\n",
      "Train Loss: 0.5995 | Validation Loss: 0.6245 | Avg Dice: 0.8298\n",
      "\n",
      "--- Epoch [21/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.8363\n",
      "WT Dice (Whole Tumor):           88.44%\n",
      "TC Dice (Tumor Core):            74.77%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.86%', '79.53%', '69.96%']\n",
      "Overall Accuracy:                98.78%\n",
      "Train Loss: 0.6073 | Validation Loss: 0.5939 | Avg Dice: 0.8363\n",
      "\n",
      "--- Epoch [22/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.8413\n",
      "WT Dice (Whole Tumor):           89.39%\n",
      "TC Dice (Tumor Core):            77.61%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.86%', '80.31%', '75.51%']\n",
      "Overall Accuracy:                98.86%\n",
      "Train Loss: 0.5871 | Validation Loss: 0.5669 | Avg Dice: 0.8413\n",
      "\n",
      "--- Epoch [23/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.8413\n",
      "WT Dice (Whole Tumor):           87.64%\n",
      "TC Dice (Tumor Core):            75.52%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.89%', '82.25%', '78.05%']\n",
      "Overall Accuracy:                98.77%\n",
      "Train Loss: 0.5745 | Validation Loss: 0.5704 | Avg Dice: 0.8413\n",
      "\n",
      "--- Epoch [24/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.8444\n",
      "WT Dice (Whole Tumor):           89.23%\n",
      "TC Dice (Tumor Core):            77.86%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.80%', '75.51%', '72.73%']\n",
      "Overall Accuracy:                98.85%\n",
      "Train Loss: 0.5650 | Validation Loss: 0.5718 | Avg Dice: 0.8444\n",
      "✅ New best model saved (best Dice: 0.8444)\n",
      "\n",
      "--- Epoch [25/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.8519\n",
      "WT Dice (Whole Tumor):           89.17%\n",
      "TC Dice (Tumor Core):            77.68%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.85%', '79.87%', '80.90%']\n",
      "Overall Accuracy:                98.90%\n",
      "Train Loss: 0.5587 | Validation Loss: 0.5345 | Avg Dice: 0.8519\n",
      "✅ New best model saved (best Dice: 0.8519)\n",
      "\n",
      "--- Epoch [26/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.8529\n",
      "WT Dice (Whole Tumor):           88.85%\n",
      "TC Dice (Tumor Core):            77.51%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.81%', '76.83%', '82.37%']\n",
      "Overall Accuracy:                98.85%\n",
      "Train Loss: 0.5567 | Validation Loss: 0.5431 | Avg Dice: 0.8529\n",
      "✅ New best model saved (best Dice: 0.8529)\n",
      "\n",
      "--- Epoch [27/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.8569\n",
      "WT Dice (Whole Tumor):           89.63%\n",
      "TC Dice (Tumor Core):            77.94%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.80%', '76.40%', '78.15%']\n",
      "Overall Accuracy:                98.91%\n",
      "Train Loss: 0.5259 | Validation Loss: 0.5342 | Avg Dice: 0.8569\n",
      "✅ New best model saved (best Dice: 0.8569)\n",
      "\n",
      "--- Epoch [28/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Results:\n",
      "Avg Dice (mean over all classes): 0.8516\n",
      "WT Dice (Whole Tumor):           89.40%\n",
      "TC Dice (Tumor Core):            77.67%\n",
      "ET Dice (Enhancing Tumor):       100.00%\n",
      "Per-Class Dice: ['99.83%', '78.79%', '80.76%']\n",
      "Overall Accuracy:                98.88%\n",
      "Train Loss: 0.5384 | Validation Loss: 0.5422 | Avg Dice: 0.8516\n",
      "\n",
      "--- Epoch [29/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 140/411 [07:03<03:53,  1.16it/s] "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from model5_ghost_upg import RefineFormer3D\n",
    "from dataset import BraTSDataset\n",
    "from optimizer import get_optimizer, get_scheduler\n",
    "from augmentation import Compose3D, RandomFlip3D, RandomRotation3D, RandomNoise3D, TumorCoreContrast\n",
    "from losses import RefineFormer3DLoss\n",
    "from config import DEVICE, IN_CHANNELS, NUM_CLASSES, BASE_LR, WEIGHT_DECAY, NUM_EPOCHS, ATTN_DROP_RATE\n",
    "\n",
    "def pad_input_for_windows(x, window_size=(2, 2, 2)):\n",
    "    _, _, D, H, W = x.shape\n",
    "    pad_d = (window_size[0] - D % window_size[0]) % window_size[0]\n",
    "    pad_h = (window_size[1] - H % window_size[1]) % window_size[1]\n",
    "    pad_w = (window_size[2] - W % window_size[2]) % window_size[2]\n",
    "    return F.pad(x, (0, pad_w, 0, pad_h, 0, pad_d))\n",
    "\n",
    "def dice_score(pred, target):\n",
    "    intersection = (pred & target).sum()\n",
    "    union = pred.sum() + target.sum()\n",
    "    return (2 * intersection + 1e-5) / (union + 1e-5)\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    dice_list, wt_dice_list, tc_dice_list, et_dice_list = [], [], [], []\n",
    "    total_correct, total_voxels = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            inputs = pad_input_for_windows(inputs, window_size=(2, 2, 2))\n",
    "            targets = targets.to(device=device, dtype=torch.long)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            preds = torch.argmax(outputs[\"main\"], dim=1)\n",
    "            total_correct += (preds == targets).sum().item()\n",
    "            total_voxels += preds.numel()\n",
    "\n",
    "            for i in range(preds.shape[0]):\n",
    "                p = preds[i].cpu().numpy()\n",
    "                g = targets[i].cpu().numpy()\n",
    "                for c in range(NUM_CLASSES):\n",
    "                    dice_list.append(dice_score(p == c, g == c))\n",
    "                wt_dice_list.append(dice_score(p > 0, g > 0))\n",
    "                tc_dice_list.append(dice_score((p == 1) | (p == 3), (g == 1) | (g == 3)))\n",
    "                et_dice_list.append(dice_score(p == 3, g == 3))\n",
    "\n",
    "    avg_loss = val_loss / len(dataloader.dataset)\n",
    "    avg_dice = np.mean(dice_list)\n",
    "    acc = total_correct / total_voxels\n",
    "\n",
    "    print(\"--- Evaluation Results:\")\n",
    "    print(f\"Avg Dice (mean over all classes): {avg_dice:.4f}\")\n",
    "    print(f\"WT Dice (Whole Tumor):           {np.mean(wt_dice_list)*100:.2f}%\")\n",
    "    print(f\"TC Dice (Tumor Core):            {np.mean(tc_dice_list)*100:.2f}%\")\n",
    "    print(f\"ET Dice (Enhancing Tumor):       {np.mean(et_dice_list)*100:.2f}%\")\n",
    "    print(f\"Per-Class Dice: {[f'{d*100:.2f}%' for d in dice_list[:NUM_CLASSES]]}\")\n",
    "    print(f\"Overall Accuracy:                {acc*100:.2f}%\")\n",
    "\n",
    "    return avg_loss, avg_dice\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, scaler):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, targets in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        inputs = pad_input_for_windows(inputs, window_size=(2, 2, 2))\n",
    "        targets = targets.to(device=device, dtype=torch.long, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(inputs)\n",
    "            if torch.isnan(outputs[\"main\"]).any():\n",
    "                print(\"⚠️ Model output contains NaNs\")\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "def main():\n",
    "    device = DEVICE\n",
    "    model = RefineFormer3D(\n",
    "        in_channels=IN_CHANNELS,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        attn_drop_rate=ATTN_DROP_RATE\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = get_optimizer(model, base_lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "    criterion = RefineFormer3DLoss()\n",
    "    scaler = GradScaler(device='cuda')\n",
    "\n",
    "    train_transform = Compose3D([\n",
    "        RandomFlip3D(p=0.5),\n",
    "        RandomRotation3D(p=0.5),\n",
    "        RandomNoise3D(p=0.3),\n",
    "        TumorCoreContrast(p=0.5, scale=1.5),\n",
    "    ])\n",
    "\n",
    "    train_dataset = BraTSDataset(\n",
    "        root_dirs=[\"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BRATS_SPLIT/train\"],\n",
    "        transform=train_transform,\n",
    "    )\n",
    "    val_dataset = BraTSDataset(\n",
    "        root_dirs=[\"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BRATS_SPLIT/val\"],\n",
    "        transform=None,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=14, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=2, shuffle=False, num_workers=12, pin_memory=True)\n",
    "\n",
    "    best_dice = 0.0\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        print(f\"\\n--- Epoch [{epoch}/{NUM_EPOCHS}] ---\")\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, scaler)\n",
    "        val_loss, val_dice = validate(model, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f} | Avg Dice: {val_dice:.4f}\")\n",
    "        torch.save(model.state_dict(), f\"checkpoint_epoch_{epoch}.pt\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_dice > best_dice:\n",
    "            best_dice = val_dice\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "            print(f\"✅ New best model saved (best Dice: {best_dice:.4f})\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"final_model.pt\")\n",
    "    print(\"Final model saved as 'final_model.pt'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7147786d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Nw=2, expected=8. Mismatch in window token count.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      3\u001b[0m     dummy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m)  \u001b[38;5;66;03m# BraTS-like shape\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdummy\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1603\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1601\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1603\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1607\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1608\u001b[0m     ):\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/model5_upgrade.py:458\u001b[0m, in \u001b[0;36mRefineFormer3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 458\u001b[0m     feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(feats)\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1603\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1601\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1603\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1607\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1608\u001b[0m     ):\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/model5_upgrade.py:329\u001b[0m, in \u001b[0;36mHierarchicalEncoder3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    327\u001b[0m x, (D, H, W) \u001b[38;5;241m=\u001b[39m embed(x)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m blocks:\n\u001b[0;32m--> 329\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;66;03m# Apply FFT global mixer only at last stage\u001b[39;00m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:  \u001b[38;5;66;03m# stage 4\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1603\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1601\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1603\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1607\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1608\u001b[0m     ):\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/model5_upgrade.py:237\u001b[0m, in \u001b[0;36mTransformerBlock3D.forward\u001b[0;34m(self, x, D, H, W)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, D, H, W):\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/_compile.py:31\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     29\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:481\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    478\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    479\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    480\u001b[0m         )\n\u001b[0;32m--> 481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    484\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    485\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:255\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    252\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 255\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/model5_upgrade.py:241\u001b[0m, in \u001b[0;36mTransformerBlock3D._forward_impl\u001b[0;34m(self, x, D, H, W)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, D, H, W):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# Attention\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# FFN Token Pruning\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     B, N, C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1603\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1601\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1603\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1607\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1608\u001b[0m     ):\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/model5_upgrade.py:151\u001b[0m, in \u001b[0;36mWindowAttention3D.forward\u001b[0;34m(self, x, D, H, W, mask)\u001b[0m\n\u001b[1;32m    149\u001b[0m Bn, heads, Nw, _ \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    150\u001b[0m expected_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m Nw \u001b[38;5;241m==\u001b[39m expected_tokens, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNw=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, expected=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Mismatch in window token count.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_bias[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_idx\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)]\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m    154\u001b[0m     expected_tokens,\n\u001b[1;32m    155\u001b[0m     expected_tokens,\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    157\u001b[0m )\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# [1, heads, Nw, Nw]\u001b[39;00m\n\u001b[1;32m    159\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn \u001b[38;5;241m+\u001b[39m bias\u001b[38;5;241m.\u001b[39mto(attn\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Nw=2, expected=8. Mismatch in window token count."
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    dummy = torch.randn(1, 4, 128, 128, 128)  # BraTS-like shape\n",
    "    output = model(dummy)\n",
    "    print(f\"Input shape: {dummy.shape}\")\n",
    "    print(f\"Output shape: {output['main'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f95f876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "_Map_base::at",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     18\u001b[0m     flops \u001b[38;5;241m=\u001b[39m FlopCountAnalysis(model, dummy_input)\n\u001b[0;32m---> 19\u001b[0m     flops_result \u001b[38;5;241m=\u001b[39m \u001b[43mflops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1e9\u001b[39m  \u001b[38;5;66;03m# Convert to GFLOPs\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ GFLOPs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflops_result\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m G\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/fvcore/nn/jit_analysis.py:248\u001b[0m, in \u001b[0;36mJitModelAnalysis.total\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtotal\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    Returns the total aggregated statistic across all operators\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m    for the requested module.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m        int : The aggregated statistic.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_analyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     module_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanonical_module_name(module_name)\n\u001b[1;32m    250\u001b[0m     total_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(stats\u001b[38;5;241m.\u001b[39mcounts[module_name]\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/fvcore/nn/jit_analysis.py:551\u001b[0m, in \u001b[0;36mJitModelAnalysis._analyze\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_trace \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_tracer_warning\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    550\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39mTracerWarning)\n\u001b[0;32m--> 551\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43m_get_scoped_trace_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aliases\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;66;03m# Assures even modules not in the trace graph are initialized to zero count\u001b[39;00m\n\u001b[1;32m    554\u001b[0m counts \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/fvcore/nn/jit_analysis.py:176\u001b[0m, in \u001b[0;36m_get_scoped_trace_graph\u001b[0;34m(module, inputs, aliases)\u001b[0m\n\u001b[1;32m    173\u001b[0m     name \u001b[38;5;241m=\u001b[39m aliases[mod]\n\u001b[1;32m    174\u001b[0m     register_hooks(mod, name)\n\u001b[0;32m--> 176\u001b[0m graph, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_get_trace_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handle \u001b[38;5;129;01min\u001b[39;00m hook_handles:\n\u001b[1;32m    179\u001b[0m     handle\u001b[38;5;241m.\u001b[39mremove()\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/jit/_trace.py:1497\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m   1496\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[0;32m-> 1497\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mONNXTracedModule\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_return_inputs_states\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/jit/_trace.py:141\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(out_vars)\n\u001b[0;32m--> 141\u001b[0m graph, out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_by_tracing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_vars\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_create_interpreter_name_lookup_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, outs[\u001b[38;5;241m0\u001b[39m], ret_inputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/jit/_trace.py:132\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    131\u001b[0m     inputs_states\u001b[38;5;241m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[0;32m--> 132\u001b[0m outs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrace_inputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    134\u001b[0m     inputs_states[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m (inputs_states[\u001b[38;5;241m0\u001b[39m], trace_inputs)\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1603\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1601\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1603\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1607\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1608\u001b[0m     ):\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1543\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1543\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/model5_upgrade.py:477\u001b[0m, in \u001b[0;36mRefineFormer3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 477\u001b[0m     feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(feats)\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1603\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1601\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1603\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1607\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1608\u001b[0m     ):\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1543\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1543\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/model5_upgrade.py:350\u001b[0m, in \u001b[0;36mHierarchicalEncoder3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    348\u001b[0m x, (D, H, W) \u001b[38;5;241m=\u001b[39m pad_feature_map_to_window(x, D, H, W, blk\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m#print(f\"[Encoder] Stage {stage_idx} Block {block_idx}, post-pad: D={D}, H={H}, W={W}\")\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# Apply FFT mixer at last stage\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m norm \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1603\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1601\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1603\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1607\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1608\u001b[0m     ):\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1543\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1543\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/model5_upgrade.py:268\u001b[0m, in \u001b[0;36mTransformerBlock3D.forward\u001b[0;34m(self, x, D, H, W)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, D, H, W):\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/_compile.py:31\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     29\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:481\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    478\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    479\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    480\u001b[0m         )\n\u001b[0;32m--> 481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    484\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    485\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/my implementations/segformer3d_upgraded/.venv/lib/python3.8/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: _Map_base::at"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
    "from model5_upgrade import RefineFormer3D\n",
    "from config import IN_CHANNELS, NUM_CLASSES\n",
    "\n",
    "# Set device to CPU for analysis\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "model = RefineFormer3D(in_channels=IN_CHANNELS, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Dummy 3D input: [Batch, Channels, Depth, Height, Width]\n",
    "dummy_input = torch.randn(1, IN_CHANNELS, 128, 128, 128).to(DEVICE)\n",
    "\n",
    "# Compute FLOPs\n",
    "with torch.no_grad():\n",
    "    flops = FlopCountAnalysis(model, dummy_input)\n",
    "    flops_result = flops.total() / 1e9  # Convert to GFLOPs\n",
    "\n",
    "# Print results\n",
    "print(f\"✅ GFLOPs: {flops_result:.2f} G\")\n",
    "print(\"✅ Parameter Count:\")\n",
    "print(parameter_count_table(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b2e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model4 import RefineFormer3D\n",
    "from thop import profile\n",
    "from torchinfo import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model with your config\n",
    "model = RefineFormer3D(\n",
    "    in_channels=4,\n",
    "    num_classes=3,\n",
    "    embed_dims=[64, 128, 320, 512],\n",
    "    depths=[2, 2, 2, 2],\n",
    "    num_heads=[1, 2, 4, 8],\n",
    "    window_sizes=[(4, 4, 4), (2, 4, 4), (2, 2, 2), (1, 2, 2)],\n",
    "    mlp_ratios=[4, 4, 4, 4],\n",
    "    decoder_channels=[256, 128, 64, 32]\n",
    ").to(device)\n",
    "\n",
    "# Dummy input\n",
    "dummy_input = torch.randn(1, 4, 128, 128, 128).to(device)\n",
    "\n",
    "# Check output shape\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "    print(\"✅ Output shape:\", output[\"main\"].shape)\n",
    "\n",
    "# Param count\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"📊 Total Parameters: {total_params / 1e6:.2f}M\")\n",
    "\n",
    "# THOP for GFLOPs\n",
    "macs, params = profile(model, inputs=(dummy_input,), verbose=False)\n",
    "print(f\"⚙️ Estimated MACs: {macs / 1e9:.2f} GFLOPs\")\n",
    "\n",
    "# Optional: torchinfo summary\n",
    "summary(model, input_size=(1, 4, 128, 128, 128), depth=4, device=device.type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b6575cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Missing segmentation files:\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_195_1/Brats17_TCIA_195_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_AAM_1/Brats17_CBICA_AAM_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ABT_1/Brats17_CBICA_ABT_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ALA_1/Brats17_CBICA_ALA_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ALT_1/Brats17_CBICA_ALT_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ALV_1/Brats17_CBICA_ALV_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ALZ_1/Brats17_CBICA_ALZ_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_AMF_1/Brats17_CBICA_AMF_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_AMU_1/Brats17_CBICA_AMU_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ANK_1/Brats17_CBICA_ANK_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_APM_1/Brats17_CBICA_APM_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_AQE_1/Brats17_CBICA_AQE_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ARR_1/Brats17_CBICA_ARR_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_ATW_1/Brats17_CBICA_ATW_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_AUC_1/Brats17_CBICA_AUC_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_AUE_1/Brats17_CBICA_AUE_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_CBICA_AZA_1/Brats17_CBICA_AZA_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_212_1/Brats17_TCIA_212_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_216_1/Brats17_TCIA_216_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_230_1/Brats17_TCIA_230_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_248_1/Brats17_TCIA_248_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_253_1/Brats17_TCIA_253_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_288_1/Brats17_TCIA_288_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_311_1/Brats17_TCIA_311_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_313_1/Brats17_TCIA_313_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_400_1/Brats17_TCIA_400_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_600_1/Brats17_TCIA_600_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_601_1/Brats17_TCIA_601_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_602_1/Brats17_TCIA_602_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_604_1/Brats17_TCIA_604_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_609_1/Brats17_TCIA_609_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_610_1/Brats17_TCIA_610_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_611_1/Brats17_TCIA_611_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_612_1/Brats17_TCIA_612_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_613_1/Brats17_TCIA_613_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_617_1/Brats17_TCIA_617_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_636_1/Brats17_TCIA_636_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_638_1/Brats17_TCIA_638_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_646_1/Brats17_TCIA_646_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_TCIA_652_1/Brats17_TCIA_652_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_UAB_3446_1/Brats17_UAB_3446_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_UAB_3454_1/Brats17_UAB_3454_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_UAB_3455_1/Brats17_UAB_3455_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_UAB_3456_1/Brats17_UAB_3456_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_UAB_3498_1/Brats17_UAB_3498_1_seg.nii\n",
      "/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData/Brats17_UAB_3499_1/Brats17_UAB_3499_1_seg.nii\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def find_missing_seg_files(validation_dir):\n",
    "    missing = []\n",
    "    for case in os.listdir(validation_dir):\n",
    "        case_path = os.path.join(validation_dir, case)\n",
    "        if os.path.isdir(case_path):\n",
    "            seg_file = os.path.join(case_path, f\"{case}_seg.nii\")\n",
    "            if not os.path.isfile(seg_file):\n",
    "                missing.append(seg_file)\n",
    "    return missing\n",
    "\n",
    "# Example usage\n",
    "validation_dir = \"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17ValidationData\"\n",
    "missing_files = find_missing_seg_files(validation_dir)\n",
    "\n",
    "if missing_files:\n",
    "    print(\"⚠️ Missing segmentation files:\")\n",
    "    for f in missing_files:\n",
    "        print(f)\n",
    "else:\n",
    "    print(\"✅ All segmentation files are present.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2202c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def check_missing_seg(root_dir):\n",
    "    missing_patients = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for dirname in dirnames:\n",
    "            seg_path = os.path.join(dirpath, dirname, f\"{dirname}_seg.nii\")\n",
    "            if not os.path.exists(seg_path):\n",
    "                missing_patients.append(dirname)\n",
    "    return missing_patients\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = \"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17TrainingData/HGG/\"\n",
    "    missing_hgg = check_missing_seg(data_dir)\n",
    "    \n",
    "    data_dir = \"/mnt/m2ssd/research project/Lightweight 3D Vision Transformers for Medical Imaging/dataset/BraTs2017/BRATS2017/Brats17TrainingData/LGG/\"\n",
    "    missing_lgg = check_missing_seg(data_dir)\n",
    "\n",
    "    print(\"Missing in HGG:\", missing_hgg)\n",
    "    print(\"Missing in LGG:\", missing_lgg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b678a36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔵 Checking environment:\n",
      "\n",
      "Torch Version        : 2.4.1+cu118\n",
      "CUDA Version         : 11.8\n",
      "cuDNN Version        : 90100\n",
      "GPU Available        : True\n",
      "Device Count         : 1\n",
      "Current Device       : 0\n",
      "Device Name          : NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "\n",
      "🟡 Checking autocast compatibility:\n",
      "❌ autocast(device_type='cuda') not supported: __init__() got an unexpected keyword argument 'device_type'\n",
      "✅ You must use: autocast() only.\n",
      "\n",
      "🟡 Checking GradScaler compatibility:\n",
      "✅ GradScaler() works (no device_type needed).\n",
      "\n",
      "✅ Environment check complete.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3847/3946993505.py:33: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
      "/tmp/ipykernel_3847/3946993505.py:42: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_environment():\n",
    "    print(\"\\n🔵 Checking environment:\\n\")\n",
    "\n",
    "    # PyTorch Version\n",
    "    print(f\"Torch Version        : {torch.__version__}\")\n",
    "\n",
    "    # CUDA Version\n",
    "    if torch.version.cuda:\n",
    "        print(f\"CUDA Version         : {torch.version.cuda}\")\n",
    "    else:\n",
    "        print(\"CUDA Version         : None (CPU Only)\")\n",
    "\n",
    "    # cuDNN Version\n",
    "    if torch.backends.cudnn.is_available():\n",
    "        print(f\"cuDNN Version        : {torch.backends.cudnn.version()}\")\n",
    "    else:\n",
    "        print(\"cuDNN Version        : None\")\n",
    "\n",
    "    # GPU Availability\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    print(f\"GPU Available        : {gpu_available}\")\n",
    "\n",
    "    if gpu_available:\n",
    "        print(f\"Device Count         : {torch.cuda.device_count()}\")\n",
    "        print(f\"Current Device       : {torch.cuda.current_device()}\")\n",
    "        print(f\"Device Name          : {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    print(\"\\n🟡 Checking autocast compatibility:\")\n",
    "\n",
    "    try:\n",
    "        with torch.cuda.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            print(\"✅ autocast(device_type='cuda', dtype=torch.float16) works!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ autocast(device_type='cuda') not supported: {e}\")\n",
    "        print(\"✅ You must use: autocast() only.\")\n",
    "\n",
    "    print(\"\\n🟡 Checking GradScaler compatibility:\")\n",
    "\n",
    "    try:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        print(\"✅ GradScaler() works (no device_type needed).\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ GradScaler() failed: {e}\")\n",
    "\n",
    "    print(\"\\n✅ Environment check complete.\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_environment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c115946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "✅ Phase 1: Prune Redundant Model Components (Baseline Compression)\n",
    "\n",
    "Objective: Identify parts of RefineFormer3D that are heavy but not critical for performance.\n",
    "\n",
    "Steps:\n",
    "\n",
    "    Use torchprofile or fvcore to inspect each block’s contribution to total FLOPs.\n",
    "\n",
    "    Replace heavier MLP/FFN blocks with:\n",
    "\n",
    "        Depthwise separable convolutions.\n",
    "\n",
    "        Low-rank approximated linear layers (e.g., nn.LinearLowRank).\n",
    "\n",
    "    Remove any redundant skip connections or high-resolution fusion blocks unless justified.\n",
    "\n",
    "Evaluation:\n",
    "\n",
    "    Re-train with minimal epochs (10–15) on a smaller subset.\n",
    "\n",
    "    Check Dice/IOU drop < 2%. If yes, continue pruning.\n",
    "\n",
    "✅ Phase 2: Switch to Efficient Attention\n",
    "\n",
    "Objective: Replace traditional attention modules with efficient alternatives.\n",
    "\n",
    "What to try:\n",
    "\n",
    "    Linear Attention (e.g., Linformer, Performer).\n",
    "\n",
    "    Axial Attention (for better locality).\n",
    "\n",
    "    Grouped Self-Attention (token grouping across z-axis).\n",
    "\n",
    "Integration:\n",
    "\n",
    "    Replace global self-attention in transformer encoder blocks with these.\n",
    "\n",
    "    Test combinations like axial in encoder, linear in bottleneck.\n",
    "\n",
    "Fallback:\n",
    "\n",
    "    If accuracy drops >3%, roll back to original attention in critical stages only.\n",
    "\n",
    "✅ Phase 3: Token Downsampling / Early Pooling\n",
    "\n",
    "Objective: Reduce spatial dimensions early in the encoder.\n",
    "\n",
    "How:\n",
    "\n",
    "    Add an aggressive Conv3d(stride=2) or pooling block right after the input.\n",
    "\n",
    "    Maintain hierarchical features through skip-connections.\n",
    "\n",
    "Tip: Pair with a lightweight decoder like SegNeXt3D or DeepLab3D.\n",
    "✅ Phase 4: Knowledge Distillation (KD)\n",
    "\n",
    "Objective: Train a compact student model to mimic your best performing RefineFormer3D.\n",
    "\n",
    "Steps:\n",
    "\n",
    "    Train the student using Dice + KL divergence from teacher’s soft logits.\n",
    "\n",
    "    Freeze teacher; use teacher’s intermediate attention maps (optional).\n",
    "\n",
    "Bonus: Try structured KD from only ET, TC, WT prediction heads.\n",
    "✅ Phase 5: Quantization & Mixed Precision Tricks\n",
    "\n",
    "Objective: Compress without architectural changes.\n",
    "\n",
    "Ideas:\n",
    "\n",
    "    Apply Post-Training Quantization (PTQ).\n",
    "\n",
    "    Try Quantization-Aware Training (QAT) for better retention of metrics.\n",
    "\n",
    "    Use torch.amp consistently.\n",
    "\n",
    "Fallback:\n",
    "\n",
    "    If quantization drops ET Dice, exclude final layer from quantization.\n",
    "\n",
    "✅ Phase 6: Neural Architecture Search (Optional but Novel)\n",
    "\n",
    "Objective: Auto-discover low-FLOP configurations.\n",
    "\n",
    "Tools:\n",
    "\n",
    "    Use nn.MetaConv3d with NASLib, Nni, or AutoFormer.\n",
    "\n",
    "    Constraints: max GFLOPs < SegFormer3D but ≥ 87% Dice.\n",
    "\n",
    "🚨 Contingency Plan\n",
    "\n",
    "    If performance improves: Save logs, metrics, configs, and prepare ablation table.\n",
    "\n",
    "    If performance drops severely:\n",
    "\n",
    "        Revert one phase back and try a lighter version of the current idea.\n",
    "\n",
    "        Use early stopping and hyperparameter search for the new config.\n",
    "\n",
    "🧪 Tracking\n",
    "\n",
    "    Maintain a spreadsheet:\n",
    "\n",
    "        Columns: Version, GFLOPs, Params, Dice, IOU, Accuracy, Training Time, Notes.\n",
    "\n",
    "    Use wandb or TensorBoard to compare training behavior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
